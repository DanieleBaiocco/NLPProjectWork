{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#TODO\n",
        "1. Costruisci un classifier in pytorch (o in tensorflow se riesci), che prende in input un abstract, espresso tramite word embedding, e ritorna, per ogni parola nell'abstract, una label che indica il tipo di classe a cui quella appartiene (BIO tagging).\n",
        " * **PROBLEMA**: capire come funziona internamente il \"Classify and Count\" di QuaPy, perche' nel tuo caso devi creare una classe simile a  \"Classify and Count\" che pero', nella funzione \"aggregate\", riduce **\"B-Claim, I-Claim, B-Evidence, I-Evidence e Outside\"** in sole 3 classi **\"Claim, Evidence e Outside\"**.\n",
        "2. Utilizza dei metodi presenti nel paper, che fanno *quantization* senza fare classification+counting\n",
        "3. Paragona i risultati di questi due approcci, su diverse metriche.\n",
        "\n",
        "# IMPORTANTE per QuaPy\n",
        "1. Vedi come fare Model Selection usando la GridSearch di QuaPy\n",
        "2. Vedi come testare i tuoi modelli, attraverso l'uso di  *natural prevalence protocol (NPP)* e  *artificial prevalence protocol (APP)*\n",
        "3. Vedi come fare dei grafici, da cui e' possibile discutere i risultati\n",
        "4. Vedi come poter usare QuaNet\n",
        "\n",
        "`QuaPy provides an implementation of QuaNet, a deep-learning-based method for performing quantification on samples of textual documents, presented in [8].8 QuaNet processes as input a list of document embeddings (see below),\n",
        "one for each unlabelled document along with their posterior probabilities generated by a probabilistic classifier. The\n",
        "list is processed by a bidirectional LSTM that generates a sample embedding (i.e., a dense representation of the entire\n",
        "sample), which is then concatenated with a vector of class prevalence estimates produced by an ensemble of simpler\n",
        "quantification methods (CC, ACC, PCC, PACC, SLD). This vector is then transformed by a set of feed-forward layers,\n",
        "followed by ReLU activations and dropout, to compute the final estimations.`\n",
        "\n",
        "QuaNet thus requires a probabilistic classifier that can provide embedded representations of the inputs. QuaPy offers\n",
        "a basic implementation of such a classifier, based on convolutional neural networks, that returns its next-to-last representation as the document embedding. The following is a working example showing how to index a textual dataset\n",
        "(see Section 3) and how to instantiate QuaNet:\n",
        "```python\n",
        "1 import quapy as qp\n",
        "2 from quapy.method.meta import QuaNet\n",
        "3 from classification.neural import NeuralClassifierTrainer, CNNnet\n",
        "4\n",
        "5 qp.environ['SAMPLE_SIZE'] = 500\n",
        "6\n",
        "7 # load the kindle dataset as plain text, and convert words\n",
        "8 # to numerical indexes\n",
        "9 dataset = qp.datasets.fetch_reviews('kindle', pickle=True)\n",
        "10 qp.data.preprocessing.index(dataset, min_df=5, inplace=True)\n",
        "11\n",
        "12 cnn = CNNnet(dataset.vocabulary_size, dataset.n_classes)\n",
        "13 learner = NeuralClassifierTrainer(cnn, device='cuda')\n",
        "14 model = QuaNet(learner, sample_size=500, device='cuda')\n",
        "```\n",
        "QuaNet paper link : Esuli, A., Moreo, A., & Sebastiani, F. (2018, October). A recurrent neural network for sentiment quantification. In Proceedings of the 27th ACM International Conference on Information and Knowledge Management (pp. 1775-1778).\n",
        "# IMPORTANTE per Preprocessing\n",
        "1. Vedi quale Word Embedding utilizzare\n",
        "2. Vedi se e come utilizzare Bert per generare degli abstract embeddings\n",
        "3. Vedi se ha senso usare altri tipi, come SciBert (che e' stato allenato su papers scientifici)\n",
        "\n",
        "\n",
        "PROBLEMA IMPORTANTISSIMO"
      ],
      "metadata": {
        "id": "l6VgU4LzLQja"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers pytorch-crf==0.7.2 torchmetrics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RJJQkHOUOnuy",
        "outputId": "86f35a02-21bc-46b4-e3ff-0ea82e394eb6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: pytorch-crf==0.7.2 in /usr/local/lib/python3.10/dist-packages (0.7.2)\n",
            "Requirement already satisfied: torchmetrics in /usr/local/lib/python3.10/dist-packages (1.5.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (2.4.1+cu121)\n",
            "Requirement already satisfied: lightning-utilities>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (0.11.8)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (75.1.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.4.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.1.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->torchmetrics) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->torchmetrics) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import sys\n",
        "import os\n",
        "import collections\n",
        "import random\n",
        "from tqdm import trange,tqdm\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import TensorDataset, DataLoader, Subset\n",
        "from transformers import BasicTokenizer, BertPreTrainedModel, BertModel, BertConfig,  BertTokenizer, get_linear_schedule_with_warmup\n",
        "from torch.optim import AdamW\n",
        "import numpy as np\n",
        "from torch import nn\n",
        "from torchcrf import CRF\n",
        "import torchmetrics"
      ],
      "metadata": {
        "id": "Gsc2QAm5S41K"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ok7lf8KG7KgF",
        "outputId": "254bdb75-427b-4df2-bb0e-d9d39889f25a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = '/content/drive/My Drive/NLPProjectWork/data/neoplasm'\n",
        "# Ensure the directory exists\n",
        "if os.path.exists(data_dir):\n",
        "    os.chdir(data_dir)\n",
        "    print(f\"Current directory: {os.getcwd()}\")\n",
        "else:\n",
        "    print(\"Directory not found.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9u-buDKd7PGe",
        "outputId": "42c9f0f1-9d58-43ae-f16c-332e4771689d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current directory: /content/drive/My Drive/NLPProjectWork/data/neoplasm\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def set_seed(seed, n_gpu):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if n_gpu > 0:\n",
        "        torch.cuda.manual_seed_all(seed)"
      ],
      "metadata": {
        "id": "HntvYy00AcdG"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_gpu = torch.cuda.device_count()\n",
        "set_seed(seed = 2, n_gpu = n_gpu)"
      ],
      "metadata": {
        "id": "Pk7HO6PT3zmK"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PREPROCESSING\n"
      ],
      "metadata": {
        "id": "EcivXELPorgq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class InputExample(object):\n",
        "    def __init__(self, guid, text, labels=None):\n",
        "        self.guid = guid\n",
        "        self.text = text\n",
        "        self.labels = labels\n",
        "\n",
        "\n",
        "class InputFeatures(object):\n",
        "    def __init__(self, input_ids, input_mask, segment_ids, label_ids, label_proba):\n",
        "        self.input_ids = input_ids\n",
        "        self.input_mask = input_mask\n",
        "        self.segment_ids = segment_ids\n",
        "        self.label_ids = label_ids\n",
        "        self.label_proba = label_proba"
      ],
      "metadata": {
        "id": "LZz7OS-1os0V"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DataProcessor(object):\n",
        "    def __init__(self):\n",
        "        self.labels = [\"X\", \"B-Claim\", \"I-Claim\", \"B-Premise\", \"I-Premise\", 'O']\n",
        "        self.label_map = self._create_label_map()\n",
        "        self.replace_labels = {\n",
        "            'B-MajorClaim': 'B-Claim',\n",
        "            'I-MajorClaim': 'I-Claim',\n",
        "        }\n",
        "\n",
        "    def _create_label_map(self):\n",
        "        label_map = collections.OrderedDict()\n",
        "        for i, label in enumerate(self.labels):\n",
        "            label_map[label] = i\n",
        "        return label_map\n",
        "\n",
        "    def get_train_examples(self, data_dir):\n",
        "        \"\"\"See base class.\"\"\"\n",
        "        return self._create_examples(\n",
        "            self.read_conll(os.path.join(data_dir, \"train.conll\"), replace=self.replace_labels), \"train\")\n",
        "\n",
        "    def get_dev_examples(self, data_dir):\n",
        "        \"\"\"See base class.\"\"\"\n",
        "        return self._create_examples(\n",
        "            self.read_conll(os.path.join(data_dir, \"dev.conll\"), replace=self.replace_labels), \"dev\")\n",
        "\n",
        "    def get_test_examples(self, data_dir, setname=\"test.conll\"):\n",
        "        \"\"\"See base class.\"\"\"\n",
        "        return self._create_examples(\n",
        "            self.read_conll(os.path.join(data_dir, setname), replace=self.replace_labels), \"test\")\n",
        "\n",
        "    def get_labels(self):\n",
        "        \"\"\" See base class.\"\"\"\n",
        "        return self.labels\n",
        "\n",
        "    def convert_labels_to_ids(self, labels):\n",
        "        idx_list = []\n",
        "        for label in labels:\n",
        "            idx_list.append(self.label_map[label])\n",
        "        return idx_list\n",
        "\n",
        "    def convert_ids_to_labels(self, idx_list):\n",
        "        labels_list = []\n",
        "        for idx in idx_list:\n",
        "            labels_list.append([key for key in self.label_map.keys() if self.label_map[key] == idx][0])\n",
        "        return labels_list\n",
        "\n",
        "    def _create_examples(self, lines, set_type):\n",
        "        \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
        "        examples = []\n",
        "        for (i, line) in enumerate(lines):\n",
        "            guid = \"%s-%s\" % (set_type, str(i))\n",
        "            text = line[0]\n",
        "            labels = line[-1]\n",
        "            examples.append(\n",
        "                InputExample(guid=guid, text=text, labels=labels))\n",
        "        return examples\n",
        "\n",
        "    def convert_examples_to_features(self, examples, max_seq_length, tokenizer,\n",
        "                                     cls_token='[CLS]',\n",
        "                                     sep_token='[SEP]'):\n",
        "        \"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"\n",
        "\n",
        "        features = []\n",
        "        for (ex_index, example) in enumerate(examples):\n",
        "            tokens_a, labels = tokenizer.tokenize_with_label_extension(example.text, example.labels,\n",
        "                                                                       copy_previous_label=True)\n",
        "\n",
        "            # Account for [CLS] and [SEP] with \"- 2\"\n",
        "            if len(tokens_a) > max_seq_length - 2:\n",
        "                tokens_a = tokens_a[:(max_seq_length - 2)]\n",
        "                labels = labels[:(max_seq_length - 2)]\n",
        "            labels = [\"X\"] + labels + [\"X\"]\n",
        "\n",
        "            tokens = [cls_token] + tokens_a + [sep_token]\n",
        "            segment_ids = [0] * len(tokens)\n",
        "            input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "            # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
        "            # tokens are attended to.\n",
        "            input_mask = [1] * len(input_ids)\n",
        "            # Zero-pad up to the sequence length.\n",
        "            padding = [0] * (max_seq_length - len(input_ids))\n",
        "            input_ids += padding\n",
        "            input_mask += padding\n",
        "            segment_ids += padding\n",
        "            label_ids = self.convert_labels_to_ids(labels)\n",
        "            # Se il mask e' None allora non fare nulla\n",
        "            # Altrimenti maska sta label\n",
        "            label_proba = self.compute_label_probadist(label_ids)\n",
        "            # leva il primo e l'ultimo elemento\n",
        "            # assert(non contiene 0 all'interno)\n",
        "            # nel caso delle predizioni se contiene 0 che devo fare? Direi di skippare tutti gli zeri dal calcolo della proba di dist\n",
        "\n",
        "            label_ids += padding\n",
        "\n",
        "            assert len(label_ids) == max_seq_length\n",
        "            assert len(input_ids) == max_seq_length\n",
        "            assert len(input_mask) == max_seq_length\n",
        "            assert len(segment_ids) == max_seq_length\n",
        "\n",
        "            features.append(\n",
        "                InputFeatures(input_ids=input_ids,\n",
        "                              input_mask=input_mask,\n",
        "                              segment_ids=segment_ids,\n",
        "                              label_ids=label_ids,\n",
        "                              label_proba=label_proba))\n",
        "        return features\n",
        "\n",
        "    def compute_label_probadist(self, label_ids, mask=None):\n",
        "        label_ids_np = np.array(label_ids)\n",
        "        if mask is not None:\n",
        "            mask_np = mask.cpu().detach().numpy()\n",
        "            label_ids_np = label_ids_np[mask_np == 1]\n",
        "        label_ids_np = label_ids_np[1:-1]\n",
        "        label_ids_np = label_ids_np[label_ids_np != 0]\n",
        "        if len(label_ids_np) == 0:\n",
        "            return [0.0, 0.0, 0.0]\n",
        "        assert (0 not in label_ids_np)\n",
        "        map = {self.label_map[\"B-Claim\"]: 0,\n",
        "               self.label_map[\"I-Claim\"]: 0,\n",
        "               self.label_map[\"B-Premise\"]: 1,\n",
        "               self.label_map[\"I-Premise\"]: 1,\n",
        "               self.label_map[\"O\"]: 2}\n",
        "        remapped_label_ids = [map[label] for label in label_ids_np]\n",
        "        counts = np.bincount(remapped_label_ids, minlength=3)\n",
        "        probabilities = counts / len(remapped_label_ids)\n",
        "        return probabilities\n",
        "\n",
        "    @classmethod\n",
        "    def features_to_dataset(cls, feature_list):\n",
        "        all_input_ids = torch.tensor([f.input_ids for f in feature_list], dtype=torch.long)\n",
        "        all_input_mask = torch.tensor([f.input_mask for f in feature_list], dtype=torch.uint8)\n",
        "        all_segment_ids = torch.tensor([f.segment_ids for f in feature_list], dtype=torch.long)\n",
        "        all_label_ids = torch.tensor([f.label_ids for f in feature_list], dtype=torch.long)\n",
        "        all_label_probas = torch.tensor([np.array(f.label_proba) for f in feature_list])\n",
        "        dataset = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids, all_label_probas)\n",
        "        return dataset\n",
        "\n",
        "    @classmethod\n",
        "    def read_conll(cls, input_file, token_number=0, token_column=1, label_column=4, replace=None):\n",
        "        \"\"\"Reads a conll type file.\"\"\"\n",
        "        with open(input_file, \"r\", encoding='utf-8') as f:\n",
        "            lines = f.readlines()\n",
        "            sentences = []\n",
        "            tokenizer = BasicTokenizer()\n",
        "            sent_tokens = []\n",
        "            sent_labels = []\n",
        "\n",
        "            for idx, line in enumerate(lines):\n",
        "\n",
        "                line = line.split('\\t')\n",
        "                # Skip the lines in which there is /n\n",
        "                if len(line) < 2:\n",
        "                    continue\n",
        "\n",
        "                # Controllare se sono all'ID numero 1, assicurandomi nel mentre che NON sia la prima iterazione\n",
        "                if idx != 0 and int(line[token_number]) == 0:\n",
        "                    assert len(sent_tokens) == len(sent_labels)\n",
        "                    if replace:\n",
        "                        sent_labels = [replace[label] if label in replace.keys() else label for label in sent_labels]\n",
        "                    sentences.append([' '.join(sent_tokens), sent_labels])\n",
        "                    sent_tokens = []\n",
        "                    sent_labels = []\n",
        "\n",
        "                token = line[token_column]\n",
        "                label = line[label_column].replace('\\n', '')\n",
        "                tokenized = tokenizer.tokenize(token)\n",
        "\n",
        "                if len(tokenized) > 1:\n",
        "                    for i in range(len(tokenized)):\n",
        "                        sent_tokens.append(tokenized[i])\n",
        "                        sent_labels.append(label)\n",
        "                else:\n",
        "                    sent_tokens.append(tokenized[0])\n",
        "                    sent_labels.append(label)\n",
        "            if sent_tokens != []:\n",
        "                assert len(sent_tokens) == len(sent_labels)\n",
        "                if replace:\n",
        "                    sent_labels = [replace[label] if label in replace.keys() else label for label in sent_labels]\n",
        "                sentences.append([' '.join(sent_tokens), sent_labels])\n",
        "        return sentences\n",
        "\n",
        "    def load_examples(self, data_dir, max_seq_length, tokenizer, evaluate=False, isval=False):\n",
        "        if evaluate:\n",
        "            examples = self.get_test_examples(data_dir)\n",
        "        elif isval:\n",
        "            examples = self.get_dev_examples(data_dir)\n",
        "        else:\n",
        "            examples = self.get_train_examples(data_dir)\n",
        "        features = self.convert_examples_to_features(examples, max_seq_length=max_seq_length, tokenizer=tokenizer)\n",
        "        dataset = self.features_to_dataset(features)\n",
        "        return dataset"
      ],
      "metadata": {
        "id": "xDVe4NSupIhP"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ExtendedBertTokenizer:\n",
        "    \"\"\"Extended tokenizer that wraps a base tokenizer.\"\"\"\n",
        "\n",
        "    def __init__(self, base_tokenizer):\n",
        "        self.tokenizer = base_tokenizer\n",
        "\n",
        "    def tokenize_with_label_extension(self, text, labels, copy_previous_label=False, extension_label='X'):\n",
        "        tok_text = self.tokenizer.tokenize(text)\n",
        "        for i in range(len(tok_text)):\n",
        "            if '##' in tok_text[i]:\n",
        "                if copy_previous_label:\n",
        "                    labels.insert(i, labels[i - 1])\n",
        "                else:\n",
        "                    labels.insert(i, extension_label)\n",
        "        return tok_text, labels\n",
        "\n",
        "    def convert_tokens_to_ids(self, tokens):\n",
        "      return self.tokenizer.convert_tokens_to_ids(tokens)"
      ],
      "metadata": {
        "id": "2xMC_rX_pQVv"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MODELS"
      ],
      "metadata": {
        "id": "ZR9Uz7YFog2Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_labels = 3  # Let's say we have 3 labels (0, 1, 2)\n",
        "batch_size = 1\n",
        "seq_length = 5\n",
        "\n",
        "emissions = torch.tensor([[[ 0.3427, -0.6740, -0.6637],\n",
        "         [-0.6571,  0.0615, -0.1830],\n",
        "         [ 0.0216,  0.1059, -0.7968],\n",
        "         [ 0.3336,  0.0276,  0.5800],\n",
        "         [ 0.3955, -0.4380, -0.3168]]])\n",
        "emissions.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K7fmAkC08yt9",
        "outputId": "36308793-2669-4c43-b3d7-1d80c6b59f73"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 5, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the CRF layer\n",
        "crf = CRF(num_labels, batch_first=True)\n",
        "\n",
        "# Create a tensor for labels (ground truth)\n",
        "# Shape: (batch_size, seq_length)\n",
        "# Using -1 for padding (not a real label)\n",
        "labels = torch.tensor([[0, 1, -1, -1, -1],\n",
        "                       ])"
      ],
      "metadata": {
        "id": "FhGKI6Bv8M_v"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss = -crf(emissions, labels, reduction='mean')  # Negative log likelihood loss\n",
        "print(\"CRF Loss:\", loss.item())\n",
        "\n",
        "# Decode the predicted labels (best path)\n",
        "decoded_labels = crf.decode(emissions)\n",
        "print(\"Decoded Labels:\", decoded_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eJU7rG7X8wTj",
        "outputId": "3a619caa-eb8e-4b5e-88a2-1bf09c845cfe"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CRF Loss: 5.336999893188477\n",
            "Decoded Labels: [[0, 1, 1, 2, 0]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchcrf/__init__.py:249: UserWarning: where received a uint8 condition tensor. This behavior is deprecated and will be removed in a future version of PyTorch. Use a boolean condition instead. (Triggered internally at ../aten/src/ATen/native/TensorCompare.cpp:530.)\n",
            "  score = torch.where(mask[i].unsqueeze(1), next_score, score)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BertForSequenceTagging(BertPreTrainedModel):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.num_labels = config.num_labels\n",
        "        self.bert = BertModel(config)\n",
        "        self.rnn = nn.GRU(config.hidden_size, config.hidden_size, batch_first=True, bidirectional=True)\n",
        "        self.crf = CRF(config.num_labels, batch_first=True)\n",
        "        self.classifier = nn.Linear(2 * config.hidden_size, config.num_labels)\n",
        "        self.custom_init_weights()\n",
        "        self.name = \"seq_tag_model\"\n",
        "\n",
        "    def custom_init_weights(self):\n",
        "        # Initialize weights of GRU\n",
        "        for name, param in self.rnn.named_parameters():\n",
        "            if 'weight' in name:\n",
        "                nn.init.xavier_uniform_(param, gain=0.01)\n",
        "            elif 'bias' in name:\n",
        "                nn.init.zeros_(param)  # Initialize biases to zero\n",
        "\n",
        "        # Initialize weights of CRF transitions\n",
        "        nn.init.normal_(self.crf.start_transitions, mean=0, std=0.1)\n",
        "        nn.init.normal_(self.crf.end_transitions, mean=0, std=0.1)\n",
        "        nn.init.normal_(self.crf.transitions, mean=0, std=0.1)\n",
        "\n",
        "        # Initialize weights of the classifier\n",
        "        nn.init.xavier_uniform_(self.classifier.weight)\n",
        "        if self.classifier.bias is not None:\n",
        "            nn.init.zeros_(self.classifier.bias)\n",
        "    def forward(\n",
        "            self,\n",
        "            input_ids=None,\n",
        "            attention_mask=None,\n",
        "            token_type_ids=None,\n",
        "            labels=None,\n",
        "            labels_proba=None\n",
        "    ):\n",
        "\n",
        "        outputs = self.bert(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids\n",
        "        )\n",
        "        sequence_output = outputs[0]\n",
        "\n",
        "        print(f\"Sequence output (BERT): {sequence_output.min()}, {sequence_output.max()}\")\n",
        "        rnn_out, _ = self.rnn(sequence_output)\n",
        "        emissions = self.classifier(rnn_out)\n",
        "\n",
        "        mask = attention_mask.bool()\n",
        "        log_likelihood = self.crf(emissions, labels, mask=mask)\n",
        "        path = self.crf.decode(emissions)\n",
        "        path = torch.LongTensor(path)\n",
        "        return (-1 * log_likelihood, emissions, path, None)"
      ],
      "metadata": {
        "id": "lKVDY9qTocBy"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BertForSequenceTaggingAndQuantification(BertPreTrainedModel):\n",
        "    def __init__(self, config, distribution_loss_fn, loss_fn_name):\n",
        "        super().__init__(config, lambda_tagging=0.0001, lambda_distribution=1.0)\n",
        "        self.num_labels = config.num_labels  # 6 labels for sequence tagging (Claim, Premise, etc.)\n",
        "        self.num_proba_labels = 3  # 3 labels for probability distribution (Claim, Premise, Not Relevant)\n",
        "\n",
        "        self.bert = BertModel(config)\n",
        "        self.rnn = nn.GRU(config.hidden_size, config.hidden_size, batch_first=True, bidirectional=True)\n",
        "        self.crf = CRF(config.num_labels, batch_first=True)\n",
        "        self.classifier = nn.Linear(2 * config.hidden_size, config.num_labels)  # For sequence tagging\n",
        "\n",
        "        # For the quantification task\n",
        "        self.projection_head = nn.Sequential(\n",
        "            nn.Linear(config.num_labels * config.max_length, 128),  # Concatenate emissions across sequence\n",
        "            nn.ReLU(),                                                  # Non-linear activation\n",
        "            nn.Linear(128, self.num_proba_labels)  # Output 3 classes (Claim, Premise, Not Relevant)\n",
        "        )\n",
        "        self.distribution_loss_fn = distribution_loss_fn\n",
        "        self.lambda_tagging = 0.0001\n",
        "        self.lambda_distribution = 1.0\n",
        "\n",
        "        self.custom_init_weights()\n",
        "        self.name = f\"seq_tag_quant_model_{loss_fn_name}\"\n",
        "\n",
        "    def custom_init_weights(self):\n",
        "        # Initialize weights of GRU\n",
        "        for name, param in self.rnn.named_parameters():\n",
        "            if 'weight' in name:\n",
        "                nn.init.xavier_uniform_(param, gain=0.01)\n",
        "            elif 'bias' in name:\n",
        "                nn.init.zeros_(param)  # Initialize biases to zero\n",
        "\n",
        "        # Initialize weights of CRF transitions\n",
        "        nn.init.normal_(self.crf.start_transitions, mean=0, std=0.1)\n",
        "        nn.init.normal_(self.crf.end_transitions, mean=0, std=0.1)\n",
        "        nn.init.normal_(self.crf.transitions, mean=0, std=0.1)\n",
        "\n",
        "        # Initialize weights of the classifier\n",
        "        nn.init.xavier_uniform_(self.classifier.weight)\n",
        "        if self.classifier.bias is not None:\n",
        "            nn.init.zeros_(self.classifier.bias)\n",
        "\n",
        "        # Initialize weights of the projection head\n",
        "        for layer in self.projection_head:\n",
        "            if isinstance(layer, nn.Linear):\n",
        "                nn.init.xavier_uniform_(layer.weight)\n",
        "                nn.init.zeros_(layer.bias)\n",
        "\n",
        "    def forward(\n",
        "            self,\n",
        "            input_ids=None,\n",
        "            attention_mask=None,\n",
        "            token_type_ids=None,\n",
        "            labels=None,\n",
        "            labels_proba=None  # True distribution of Claim, Premise, Not Relevant\n",
        "    ):\n",
        "        outputs = self.bert(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids\n",
        "        )\n",
        "        sequence_output = outputs[0]  # Shape: (batch_size, sequence_length, hidden_size)\n",
        "\n",
        "        rnn_out, _ = self.rnn(sequence_output)  # Shape: (batch_size, sequence_length, 2 * hidden_size)\n",
        "        emissions = self.classifier(rnn_out)    # Shape: (batch_size, sequence_length, num_labels)\n",
        "\n",
        "        mask = attention_mask.bool()\n",
        "        log_likelihood = self.crf(emissions, labels, mask=mask)  # Sequence tagging loss\n",
        "        path = self.crf.decode(emissions)  # CRF-decoded sequence\n",
        "        path = torch.LongTensor(path)\n",
        "\n",
        "        # Part 2: Quantification (Predict probability distribution)\n",
        "\n",
        "        # 1. Flatten emissions along the sequence dimension (this is where we avoid pooling)\n",
        "        # Emissions shape: (batch_size, sequence_length, num_labels)\n",
        "        # After flattening: (batch_size, sequence_length * num_labels)\n",
        "        flattened_emissions = emissions.view(emissions.size(0), -1)  # Concatenates emissions along sequence\n",
        "\n",
        "        # 2. Non-linear projection to 3-class probability distribution\n",
        "        predicted_distribution = self.projection_head(flattened_emissions)  # Shape: (batch_size, num_proba_labels)\n",
        "\n",
        "        # 3. Apply softmax to get a valid probability distribution\n",
        "        predicted_distribution = nn.Softmax(dim=-1)(predicted_distribution)\n",
        "\n",
        "        # Loss for the distribution prediction (e.g., KLDiv or MAE)\n",
        "        distribution_loss = None\n",
        "        # Use Kullback-Leibler divergence or Mean Absolute Error\n",
        "        distribution_loss = distribution_loss_fn(predicted_distribution.log(), labels_proba)\n",
        "\n",
        "        total_loss =self.lambda_tagging* -log_likelihood + self.lambda_distribution * distribution_loss  # Sequence tagging loss\n",
        "\n",
        "        return (total_loss, emissions, path, predicted_distribution)"
      ],
      "metadata": {
        "id": "zWbzpF3I-jFi"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ClassificationAndCounting:\n",
        "    def __init__(self, learner, processor: DataProcessor):\n",
        "        self.learner = learner\n",
        "        self.processor = processor\n",
        "        self.name = \"bert_classify_and_count\"\n",
        "\n",
        "    def forward(\n",
        "            self,\n",
        "            input_ids=None,\n",
        "            attention_mask=None,\n",
        "            token_type_ids=None,\n",
        "            labels=None,\n",
        "            labels_proba=None\n",
        "    ):\n",
        "\n",
        "        loss, emissions, path, _ = self.learner(input_ids, attention_mask, token_type_ids, labels, labels_proba)\n",
        "\n",
        "        proba_dists = self.aggregate(path, attention_mask)\n",
        "        return loss, emissions, path, proba_dists\n",
        "\n",
        "    def aggregate(self, predictions, attention_mask):\n",
        "        all_probabilities = []\n",
        "        for sequence, mask in zip(predictions, attention_mask):\n",
        "            label_proba = self.processor.compute_label_probadist(sequence, mask=mask)\n",
        "            all_probabilities.append(label_proba)\n",
        "        return np.array(all_probabilities)\n",
        "\n",
        "    # Wrapper methods mimicking PyTorch model behavior\n",
        "    def train(self, mode=True):\n",
        "        self.learner.train(mode)\n",
        "\n",
        "    def eval(self):\n",
        "        self.learner.eval()\n",
        "\n",
        "    def state_dict(self, *args, **kwargs):\n",
        "        return self.learner.state_dict(*args, **kwargs)\n",
        "\n",
        "    def load_state_dict(self, *args, **kwargs):\n",
        "        return self.learner.load_state_dict(*args, **kwargs)\n",
        "\n",
        "    def to(self, *args, **kwargs):\n",
        "        self.learner.to(*args, **kwargs)\n",
        "\n",
        "    def parameters(self):\n",
        "        return self.learner.parameters()\n",
        "\n",
        "    def named_parameters(self):\n",
        "        return self.learner.named_parameters()\n",
        "\n",
        "    def zero_grad(self):\n",
        "        self.learner.zero_grad()\n",
        "\n",
        "    def __call__(self, *args, **kwargs):\n",
        "        return self.forward(*args, **kwargs)"
      ],
      "metadata": {
        "id": "2cbm6HJwoU1P"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BertForLabelDistribution(BertPreTrainedModel):\n",
        "    def __init__(self, config, loss_fn, loss_fn_name):\n",
        "        super().__init__(config)\n",
        "        self.num_labels = config.num_labels\n",
        "        self.bert = BertModel(config)\n",
        "        self.rnn = nn.GRU(config.hidden_size, config.hidden_size, batch_first=True, bidirectional=True)\n",
        "        self.classifier = nn.Linear(2 * config.hidden_size, config.num_labels)\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "        self.init_weights()\n",
        "        self.loss_fn = loss_fn\n",
        "        self.name = f\"bert_quantify_{loss_fn_name}\"\n",
        "        self.custom_init_weights()\n",
        "\n",
        "    def custom_init_weights(self):\n",
        "        # Initialize weights of GRU\n",
        "        for name, param in self.rnn.named_parameters():\n",
        "            if 'weight' in name:\n",
        "                nn.init.xavier_uniform_(param, gain=0.01)\n",
        "            elif 'bias' in name:\n",
        "                nn.init.zeros_(param)  # Initialize biases to zero\n",
        "\n",
        "        # Initialize weights of the classifier\n",
        "        nn.init.xavier_uniform_(self.classifier.weight)\n",
        "        if self.classifier.bias is not None:\n",
        "            nn.init.zeros_(self.classifier.bias)\n",
        "\n",
        "    def forward(\n",
        "            self,\n",
        "            input_ids=None,\n",
        "            attention_mask=None,\n",
        "            token_type_ids=None,\n",
        "            labels=None,\n",
        "            labels_proba=None,\n",
        "    ):\n",
        "        outputs = self.bert(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids\n",
        "        )\n",
        "        sequence_output = outputs[0]  # [batch_size, seq_len, hidden_size]\n",
        "        rnn_out, _ = self.rnn(sequence_output)  # [batch_size, seq_len, 2*hidden_size]\n",
        "        emissions = self.classifier(rnn_out)  # [batch_size, seq_len, num_labels]\n",
        "        cls_emissions = emissions[:, 0, :]  # [batch_size, num_labels]\n",
        "        # posos calcolare la loss e dopo la softmax\n",
        "        cls_probs = self.softmax(cls_emissions)  # [batch_size, num_labels]\n",
        "        loss = self.loss_fn(cls_probs, labels_proba)\n",
        "        return loss, emissions, None, cls_probs"
      ],
      "metadata": {
        "id": "dm_FPtFyocz_"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TRAIN/EVALUATE"
      ],
      "metadata": {
        "id": "bxO6U0y7okfe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_quantization_preds_labels(inputs, outputs):\n",
        "    _, _, _, pred_proba = outputs\n",
        "    pred_proba = torch.tensor(pred_proba).to(device)\n",
        "    labels_proba = inputs[\"labels_proba\"]\n",
        "    return pred_proba,labels_proba\n",
        "\n",
        "def get_classf_preds_labels(inputs, outputs):\n",
        "    loss, emissions, path, _ = outputs\n",
        "    batch_logits = path.detach().cpu().numpy().flatten()\n",
        "    batch_labels = inputs[\"labels\"].detach().cpu().numpy().flatten()\n",
        "    attention_mask = inputs[\"attention_mask\"].detach().cpu().numpy().flatten()\n",
        "    valid_batch_logits = batch_logits[attention_mask == 1]\n",
        "    valid_batch_labels = batch_labels[attention_mask == 1]\n",
        "    return torch.tensor(valid_batch_logits), torch.tensor(valid_batch_labels)"
      ],
      "metadata": {
        "id": "ax6nmcdksLb5"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mae_loss(pred_probas, label_probas):\n",
        "    loss = nn.L1Loss(reduction=\"mean\")\n",
        "    return loss(pred_probas, label_probas)\n",
        "\n",
        "def kldiv_loss(pred_probas, label_probas):\n",
        "    kl_loss = nn.KLDivLoss(reduction=\"batchmean\")\n",
        "    pred_probas_log = torch.log(pred_probas)\n",
        "    output = kl_loss(pred_probas_log, label_probas)\n",
        "    return output"
      ],
      "metadata": {
        "id": "f7Wrj51vqmDC"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_batch(batch, device, model, gen_preds_labels_fn, eval=False):\n",
        "    model.eval() if eval else model.train()\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    inputs = {\"input_ids\": batch[0], \"attention_mask\": batch[1], \"token_type_ids\": batch[2], \"labels\": batch[3],\n",
        "              \"labels_proba\": batch[4]}\n",
        "    with torch.no_grad() if eval else torch.enable_grad():\n",
        "        outputs = model(**inputs)\n",
        "    preds, labels = gen_preds_labels_fn(inputs, outputs)\n",
        "    return outputs, preds, labels"
      ],
      "metadata": {
        "id": "yPgB9VI1nkns"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(device, model, eval_dataset, eval_batch_size, metrics, gen_preds_labels_fn):\n",
        "    eval_dataloader = DataLoader(eval_dataset, batch_size=eval_batch_size)\n",
        "    epoch_loss_sum = 0.0\n",
        "    for metric in metrics:\n",
        "        metric.reset()\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
        "            outputs, preds, labels = process_batch(batch, device, model, gen_preds_labels_fn, eval=True)\n",
        "            loss = outputs[0]\n",
        "            epoch_loss_sum += loss.item()\n",
        "            for metric in metrics:\n",
        "                if metric.__class__.__name__ == \"KLDivergence\":\n",
        "                      labels = labels.clamp(min=1e-9)\n",
        "                      preds = preds.clamp(min=1e-9)\n",
        "                metric.update(preds, labels)\n",
        "    eval_loss = epoch_loss_sum / len(eval_dataloader)\n",
        "    eval_metrics = {metric.__class__.__name__: metric.compute().item() for metric in metrics}\n",
        "    return eval_loss, eval_metrics"
      ],
      "metadata": {
        "id": "QOC4shMInjVE"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def update_metrics(history, metrics, metric_type, epoch):\n",
        "    for metric_name, metric_value in metrics.items():\n",
        "        if epoch == 0:\n",
        "            history[metric_type][metric_name] = [metric_value]\n",
        "        else:\n",
        "            history[metric_type][metric_name].append(metric_value)\n",
        "        print(f\"{metric_name}: {metric_value:.4f}\")\n",
        "def train(\n",
        "        device,\n",
        "        train_dataset,\n",
        "        model,\n",
        "        eval_dataset,\n",
        "        generate_preds_labels_fn,\n",
        "        metrics,\n",
        "        num_train_epochs=30,\n",
        "        train_batch_size=2,\n",
        "        eval_batch_size=32,\n",
        "        weight_decay=0.3,\n",
        "        learning_rate=2e-5,\n",
        "        adam_epsilon=1e-8,\n",
        "        warmup_steps=5,\n",
        "        max_grad_norm = 3.0\n",
        "):\n",
        "    history = {\n",
        "        'train_loss': [],\n",
        "        'train_metrics': {},\n",
        "        'eval_loss': [],\n",
        "        'eval_metrics': {}\n",
        "    }\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=train_batch_size)\n",
        "    num_steps_per_epoch = len(train_dataloader)\n",
        "    t_total = num_steps_per_epoch * num_train_epochs\n",
        "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
        "    optimizer_grouped_parameters = [\n",
        "        {\n",
        "            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
        "            \"weight_decay\": weight_decay,\n",
        "        },\n",
        "        {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n",
        "    ]\n",
        "    optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate, eps=adam_epsilon)\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer, num_warmup_steps=warmup_steps, num_training_steps=t_total\n",
        "    )\n",
        "    epochs_trained = 0\n",
        "\n",
        "    train_iterator = trange(epochs_trained, int(num_train_epochs), desc=\"Epoch\")\n",
        "    print(f\"TRAINING MODEL {model.name}\")\n",
        "    for epoch in train_iterator:\n",
        "        epoch_loss_sum = 0.0\n",
        "        epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\")\n",
        "        for step, batch in enumerate(epoch_iterator):\n",
        "            optimizer.zero_grad()\n",
        "            outputs, preds, labels = process_batch(batch, device, model, generate_preds_labels_fn, eval=False)\n",
        "            loss = outputs[0]\n",
        "            print(f\"SINGLE LOSS: {loss}\")\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "            epoch_loss_sum += loss.item()\n",
        "            for metric in metrics:\n",
        "                if metric.__class__.__name__ == \"KLDivergence\":\n",
        "                      labels = labels.clamp(min=1e-9)\n",
        "                      preds = preds.clamp(min=1e-9)\n",
        "                metric.update(preds, labels)\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "        print(f\"CUMULATIVE LOSSES: {epoch_loss_sum}\")\n",
        "        epoch_loss = epoch_loss_sum / num_steps_per_epoch\n",
        "        history['train_loss'].append(epoch_loss)\n",
        "        epoch_metrics = {metric.__class__.__name__: metric.compute().item() for metric in metrics}\n",
        "        eval_loss, eval_metrics = evaluate(device, model, eval_dataset, eval_batch_size, metrics,\n",
        "                                           generate_preds_labels_fn)\n",
        "        history['eval_loss'].append(eval_loss)\n",
        "        print(f\"Epoch {epoch + 1}/{num_train_epochs}\")\n",
        "        print(f\"Train Loss: {epoch_loss:.4f}\")\n",
        "        update_metrics(history, epoch_metrics, 'train_metrics', epoch)\n",
        "        print(f\"Eval Loss: {eval_loss:.4f}\")\n",
        "        update_metrics(history, eval_metrics, 'eval_metrics', epoch)\n",
        "    return history"
      ],
      "metadata": {
        "id": "MYd-aQQxngu_"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RESULTS"
      ],
      "metadata": {
        "id": "dedHEP35pXxo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name_or_path = 'allenai/scibert_scivocab_uncased'\n",
        "do_lower_case = True\n",
        "base_tokenizer = BertTokenizer.from_pretrained(model_name_or_path, do_lower_case=do_lower_case)\n",
        "extended_tokenizer = ExtendedBertTokenizer(base_tokenizer)"
      ],
      "metadata": {
        "id": "JrLrfYxWVu6d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1ddc8b3-8814-413e-a128-eedc02bae869"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "dataprocessor = DataProcessor()\n",
        "max_seq_length = 510"
      ],
      "metadata": {
        "id": "ouaGaD9nV0pf"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = dataprocessor.load_examples(data_dir=data_dir, max_seq_length=max_seq_length,\n",
        "                        tokenizer=extended_tokenizer)\n",
        "val_ds = dataprocessor.load_examples( data_dir=data_dir, max_seq_length=max_seq_length,\n",
        "                        tokenizer=extended_tokenizer, isval=True)\n",
        "test_ds = dataprocessor.load_examples(data_dir=data_dir, max_seq_length=max_seq_length,\n",
        "                        tokenizer=extended_tokenizer, evaluate=True)"
      ],
      "metadata": {
        "id": "RPJE5K_VWAo-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2364833e-8305-463e-c280-cb0618833c8f"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-7f4511887a23>:134: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
            "  all_label_probas = torch.tensor([np.array(f.label_proba) for f in feature_list])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CREATE CLASSIFICATION+COUNT MODEL"
      ],
      "metadata": {
        "id": "Yc3a-rg6WDhX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_class_labels = len(dataprocessor.get_labels())\n",
        "\n",
        "\"\"\"\n",
        "seq_tagging_config = BertConfig.from_pretrained(\n",
        "    model_name_or_path,\n",
        "    num_labels=num_class_labels\n",
        ")\n",
        "seq_tagging_model = BertForSequenceTagging.from_pretrained(\n",
        "    model_name_or_path,\n",
        "    config=seq_tagging_config\n",
        ")\n",
        "multiclass_metrics = [torchmetrics.Accuracy(num_classes=6, average='macro', task = \"multiclass\").to(device), torchmetrics.F1Score(num_classes=6, average='macro', task = \"multiclass\").to(device)]\n",
        "history = train(device=device, train_dataset=train_ds, model=seq_tagging_model, eval_dataset=val_ds, generate_preds_labels_fn=get_classf_preds_labels, metrics=multiclass_metrics)\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "collapsed": true,
        "id": "a1qmzJ8gWLIA",
        "outputId": "9849fe35-47f5-466d-d158-e07d1e7df73e"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nseq_tagging_config = BertConfig.from_pretrained(\\n    model_name_or_path,\\n    num_labels=num_class_labels\\n)\\nseq_tagging_model = BertForSequenceTagging.from_pretrained(\\n    model_name_or_path,\\n    config=seq_tagging_config\\n)\\nmulticlass_metrics = [torchmetrics.Accuracy(num_classes=6, average=\\'macro\\', task = \"multiclass\").to(device), torchmetrics.F1Score(num_classes=6, average=\\'macro\\', task = \"multiclass\").to(device)]\\nhistory = train(device=device, train_dataset=train_ds, model=seq_tagging_model, eval_dataset=val_ds, generate_preds_labels_fn=get_classf_preds_labels, metrics=multiclass_metrics)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classify_count_config = BertConfig.from_pretrained(\n",
        "    model_name_or_path,\n",
        "    num_labels=num_class_labels)\n",
        "classify_count_base_model = BertForSequenceTagging.from_pretrained(\n",
        "    model_name_or_path,\n",
        "    config=classify_count_config\n",
        ")\n",
        "classify_and_count_model = ClassificationAndCounting(learner=classify_count_base_model, processor=dataprocessor)\n",
        "classify_and_count_model.to(device)\n",
        "dist_metrics = [torchmetrics.KLDivergence(log_prob=False, reduction=\"mean\").to(device),\n",
        "                torchmetrics.MeanAbsoluteError().to(device),\n",
        "                torchmetrics.MeanSquaredError().to(device)]\n",
        "history = train(device=device, train_dataset=train_ds, model=classify_and_count_model, eval_dataset=val_ds, generate_preds_labels_fn=get_quantization_preds_labels, metrics=dist_metrics)"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 758
        },
        "id": "ppMVCxKcGSAT",
        "outputId": "5430392f-b80c-4e1e-ce69-326510c66244"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceTagging were not initialized from the model checkpoint at allenai/scibert_scivocab_uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'crf.end_transitions', 'crf.start_transitions', 'crf.transitions', 'rnn.bias_hh_l0', 'rnn.bias_hh_l0_reverse', 'rnn.bias_ih_l0', 'rnn.bias_ih_l0_reverse', 'rnn.weight_hh_l0', 'rnn.weight_hh_l0_reverse', 'rnn.weight_ih_l0', 'rnn.weight_ih_l0_reverse']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Epoch:   0%|          | 0/30 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAINING MODEL bert_classify_and_count\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Iteration:   0%|          | 0/175 [00:00<?, ?it/s]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequence output (BERT): -15.10572624206543, 16.894390106201172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchcrf/__init__.py:305: UserWarning: where received a uint8 condition tensor. This behavior is deprecated and will be removed in a future version of PyTorch. Use a boolean condition instead. (Triggered internally at ../aten/src/ATen/native/TensorCompare.cpp:530.)\n",
            "  score = torch.where(mask[i].unsqueeze(1), next_score, score)\n",
            "<ipython-input-13-01789ea52e69>:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  return torch.tensor(pred_proba).to(device),labels_proba\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SINGLE LOSS: 1.5419175033895463e+35\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Iteration:   1%|          | 1/175 [00:01<05:26,  1.88s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequence output (BERT): -13.15503215789795, 16.602663040161133\n",
            "SINGLE LOSS: 1.2212381484161545e+35\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Iteration:   1%|          | 2/175 [00:02<03:37,  1.26s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequence output (BERT): -17.15949821472168, 16.852718353271484\n",
            "SINGLE LOSS: 1.3150639018032658e+35\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Iteration:   2%|         | 3/175 [00:03<03:01,  1.05s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequence output (BERT): -16.76675033569336, 16.991296768188477\n",
            "SINGLE LOSS: 1.245897715928313e+35\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Iteration:   2%|         | 4/175 [00:04<02:43,  1.05it/s]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequence output (BERT): -14.336128234863281, 15.983817100524902\n",
            "SINGLE LOSS: 1.4710946566364202e+35\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Iteration:   2%|         | 4/175 [00:04<03:33,  1.25s/it]\n",
            "Epoch:   0%|          | 0/30 [00:04<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-d80c99ee7210>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m                 \u001b[0mtorchmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMeanAbsoluteError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m                 torchmetrics.MeanSquaredError().to(device)]\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclassify_and_count_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerate_preds_labels_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_quantization_preds_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdist_metrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-17-b2b49870003b>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(device, train_dataset, model, eval_dataset, generate_preds_labels_fn, metrics, num_train_epochs, train_batch_size, eval_batch_size, weight_decay, learning_rate, adam_epsilon, warmup_steps, max_grad_norm)\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"SINGLE LOSS: {loss}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_grad_norm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0mepoch_loss_sum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    519\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m             )\n\u001b[0;32m--> 521\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    287\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    290\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    767\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 769\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    770\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    771\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import pickle\n",
        "\n",
        "# Create directory if it doesn't exist\n",
        "save_dir = \"content/save\"\n",
        "classify_and_counnt_dir = os.path.join(save_dir, \"classify_and_count\")\n",
        "os.makedirs(classify_and_counnt_dir, exist_ok=True)\n",
        "\n",
        "# Save model's state_dict\n",
        "torch.save(classify_and_count_model.state_dict(), os.path.join(classify_and_counnt_dir, 'classify_and_count_model.pth'))\n",
        "\n",
        "# Save the configuration (if needed)\n",
        "classify_count_config.save_pretrained(classify_and_counnt_dir)"
      ],
      "metadata": {
        "id": "DVwZluSa9EXI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the configuration\n",
        "classify_count_config = BertConfig.from_pretrained(classify_and_counnt_dir)\n",
        "\n",
        "# Load the model and its state_dict\n",
        "classify_count_base_model = BertForSequenceTagging.from_pretrained(\n",
        "    model_name_or_path,  # Replace this with the same model path you used for saving\n",
        "    config=classify_count_config\n",
        ")\n",
        "\n",
        "# Initialize the ClassificationAndCounting model\n",
        "classify_and_count_model = ClassificationAndCounting(learner=classify_count_base_model, processor=dataprocessor)\n",
        "\n",
        "# Load the state dictionary\n",
        "classify_and_count_model.load_state_dict(torch.load(os.path.join(classify_and_counnt_dir, 'classify_and_count_model.pth')))\n",
        "\n",
        "# Move model to the appropriate device (CPU/GPU)\n",
        "classify_and_count_model.to(device)\n"
      ],
      "metadata": {
        "id": "w6bhTZR2-Gmy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "How would you use THIS idea of predicting each example, and then using this prediction to then compute a distribution, BUT in a different scenario:\n",
        "In a scenario in which you have a dataset composed of abstracts, and where I want to predict, for each abstract, a probability distribution that tells me how much of the content of the abstract is a Claim, how much is a Premise and how much is Not Relevant. I thought that maybe a solution similar to QuaNet, would be to predict, for each token in the abstract a class (so we are at the beginning solving a sequence tagging task), and then to use that prediction, concatenated with all the other predictions, to predict a probability distribution. So I imagine the loss to be a sum of the single losses of the sequence tagging task, plus the loss of the kldivergence (or the mae) of the predicted distribution and the true distribution"
      ],
      "metadata": {
        "id": "t3ls9BOsxyOB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eval_loss, eval_metrics = evaluate(device, classify_and_count_model, test_ds, eval_batch_size = 32, metrics = dist_metrics,\n",
        "                                           gen_preds_labels_fn  = get_quantization_preds_labels)"
      ],
      "metadata": {
        "id": "Vep29hJq8g7z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_loss"
      ],
      "metadata": {
        "id": "TCDq6yOH-1o4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_metrics"
      ],
      "metadata": {
        "id": "CuJj91HZ8uZq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# metrics = [torchmetrics.Accuracy(num_classes=3, average='macro', task = \"multiclass\"),\n",
        "#      torchmetrics.F1Score(num_classes=3, average='macro', task = \"multiclass\")]\n",
        "# history = train(device=device, train_dataset=train_ds, model=wrapper, eval_dataset=val_ds,\n",
        "#                generate_preds_labels_fn=classification_preds_labels_fn, metrics=metrics)\n",
        "# _ , test_metric = evaluate(device = device, model = model,eval_dataset= test_ds, eval_batch_size= 32,\n",
        "# gen_preds_labels_fn=classification_preds_labels_fn)\n",
        "losses = {\"kldiv\": kldiv_loss,\n",
        "          \"mae\" : mae_loss}\n",
        "quantify_models = []\n",
        "for loss_name, loss in losses.items():\n",
        "    num_quantify_classes = 3\n",
        "    quantify_config = BertConfig.from_pretrained(\n",
        "        model_name_or_path,\n",
        "        num_labels=num_quantify_classes)\n",
        "    quantify_model = BertForLabelDistribution.from_pretrained(model_name_or_path, config=quantify_config, loss_fn=loss, loss_fn_name =loss_name )\n",
        "    quantify_model.to(device)\n",
        "    quantify_models.append(quantify_model)\n",
        "\n",
        "histories = {}\n",
        "test_metrics = {}\n",
        "for quantify_model in quantify_models:\n",
        "    qua_metrics = [torchmetrics.KLDivergence(log_prob=False, reduction=\"mean\").to(device),\n",
        "                torchmetrics.MeanAbsoluteError().to(device),\n",
        "                torchmetrics.MeanSquaredError().to(device)]\n",
        "    history = train(device=device, train_dataset=train_ds, model=quantify_model, eval_dataset=val_ds,\n",
        "                    generate_preds_labels_fn=get_quantization_preds_labels, metrics=qua_metrics, train_batch_size = 5)\n",
        "    histories[quantify_model.name] = history\n",
        "    test_metric = evaluate(device, quantify_model, test_ds, eval_batch_size=32, metrics=qua_metrics,\n",
        "                            gen_preds_labels_fn=get_quantization_preds_labels)\n",
        "    test_metrics[quantify_model.name] = test_metric"
      ],
      "metadata": {
        "id": "fEpnFYHHpZSp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "c8332e96-e316-461c-8692-1ef74f52141a"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-88-1836afa3bf42>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mmodel_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         num_labels=num_quantify_classes)\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mquantify_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertForLabelDistribution\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquantify_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn_name\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mloss_name\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mquantify_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mquantify_models\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquantify_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3758\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_sharded\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mstate_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3759\u001b[0m                 \u001b[0;31m# Time to load the checkpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3760\u001b[0;31m                 \u001b[0mstate_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresolved_archive_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3761\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3762\u001b[0m             \u001b[0;31m# set dtype to instantiate the model under:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(checkpoint_file, is_quantized)\u001b[0m\n\u001b[1;32m    572\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"mmap\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m         \u001b[0mweights_only_kwarg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"weights_only\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_torch_greater_or_equal_than_1_13\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 574\u001b[0;31m         return torch.load(\n\u001b[0m\u001b[1;32m    575\u001b[0m             \u001b[0mcheckpoint_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m             \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1109\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mweights_only\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1110\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1111\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0m_legacy_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_weights_only_unpickler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1112\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnpicklingError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_get_wo_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_legacy_load\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1355\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdeserialized_objects\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1356\u001b[0m             \u001b[0mtyped_storage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeserialized_objects\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1357\u001b[0;31m             typed_storage._untyped_storage._set_from_file(\n\u001b[0m\u001b[1;32m   1358\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf_should_read_directly\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1359\u001b[0m                 torch._utils._element_size(typed_storage.dtype))\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classify_count_config = BertConfig.from_pretrained(\n",
        "    model_name_or_path,\n",
        "    num_labels=num_class_labels)\n",
        "classify_count_config.max_length = max_seq_length\n",
        "sequence_tag_quant_model = BertForSequenceTaggingAndQuantification.from_pretrained(\n",
        "    model_name_or_path,\n",
        "    config=classify_count_config,\n",
        "    distribution_loss_fn = kldiv_loss,\n",
        "    loss_fn_name = \"kldiv\"\n",
        ")\n",
        "sequence_tag_quant_model.to(device)\n",
        "dist_metrics = [torchmetrics.KLDivergence(log_prob=False, reduction=\"mean\").to(device),\n",
        "                torchmetrics.MeanAbsoluteError().to(device),\n",
        "                torchmetrics.MeanSquaredError().to(device)]\n",
        "history = train(device=device, train_dataset=train_ds, model=sequence_tag_quant_model, eval_dataset=val_ds, generate_preds_labels_fn=get_quantization_preds_labels, metrics=dist_metrics)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 446
        },
        "id": "UTQdzjPzFLF5",
        "outputId": "a3825d3d-67ff-4109-e38d-df8f25646a21"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceTaggingAndQuantification were not initialized from the model checkpoint at allenai/scibert_scivocab_uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'crf.end_transitions', 'crf.start_transitions', 'crf.transitions', 'projection_head.0.bias', 'projection_head.0.weight', 'projection_head.2.bias', 'projection_head.2.weight', 'rnn.bias_hh_l0', 'rnn.bias_hh_l0_reverse', 'rnn.bias_ih_l0', 'rnn.bias_ih_l0_reverse', 'rnn.weight_hh_l0', 'rnn.weight_hh_l0_reverse', 'rnn.weight_ih_l0', 'rnn.weight_ih_l0_reverse']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Epoch:   0%|          | 0/30 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAINING MODEL seq_tag_quant_model_kldiv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Iteration:   0%|          | 0/175 [00:06<?, ?it/s]\n",
            "Epoch:   0%|          | 0/30 [00:06<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'distribution_loss_fn' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-24e2b4dc2997>\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0mtorchmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMeanAbsoluteError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                 torchmetrics.MeanSquaredError().to(device)]\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msequence_tag_quant_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerate_preds_labels_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_quantization_preds_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdist_metrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-21-b2b49870003b>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(device, train_dataset, model, eval_dataset, generate_preds_labels_fn, metrics, num_train_epochs, train_batch_size, eval_batch_size, weight_decay, learning_rate, adam_epsilon, warmup_steps, max_grad_norm)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m             \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerate_preds_labels_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"SINGLE LOSS: {loss}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-f91d60405458>\u001b[0m in \u001b[0;36mprocess_batch\u001b[0;34m(batch, device, model, gen_preds_labels_fn, eval)\u001b[0m\n\u001b[1;32m      5\u001b[0m               \"labels_proba\": batch[4]}\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0meval\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_preds_labels_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-38-3b288201231f>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, labels, labels_proba)\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0mdistribution_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;31m# Use Kullback-Leibler divergence or Mean Absolute Error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0mdistribution_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdistribution_loss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted_distribution\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_proba\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlambda_tagging\u001b[0m\u001b[0;34m*\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mlog_likelihood\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlambda_distribution\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdistribution_loss\u001b[0m  \u001b[0;31m# Sequence tagging loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'distribution_loss_fn' is not defined"
          ]
        }
      ]
    }
  ]
}