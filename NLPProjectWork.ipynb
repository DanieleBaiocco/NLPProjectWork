{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#TODO\n",
        "1. Costruisci un classifier in pytorch (o in tensorflow se riesci), che prende in input un abstract, espresso tramite word embedding, e ritorna, per ogni parola nell'abstract, una label che indica il tipo di classe a cui quella appartiene (BIO tagging).\n",
        " * **PROBLEMA**: capire come funziona internamente il \"Classify and Count\" di QuaPy, perche' nel tuo caso devi creare una classe simile a  \"Classify and Count\" che pero', nella funzione \"aggregate\", riduce **\"B-Claim, I-Claim, B-Evidence, I-Evidence e Outside\"** in sole 3 classi **\"Claim, Evidence e Outside\"**.\n",
        "2. Utilizza dei metodi presenti nel paper, che fanno *quantization* senza fare classification+counting\n",
        "3. Paragona i risultati di questi due approcci, su diverse metriche.\n",
        "\n",
        "# IMPORTANTE per QuaPy\n",
        "1. Vedi come fare Model Selection usando la GridSearch di QuaPy\n",
        "2. Vedi come testare i tuoi modelli, attraverso l'uso di  *natural prevalence protocol (NPP)* e  *artificial prevalence protocol (APP)*\n",
        "3. Vedi come fare dei grafici, da cui e' possibile discutere i risultati\n",
        "4. Vedi come poter usare QuaNet\n",
        "\n",
        "`QuaPy provides an implementation of QuaNet, a deep-learning-based method for performing quantification on samples of textual documents, presented in [8].8 QuaNet processes as input a list of document embeddings (see below),\n",
        "one for each unlabelled document along with their posterior probabilities generated by a probabilistic classifier. The\n",
        "list is processed by a bidirectional LSTM that generates a sample embedding (i.e., a dense representation of the entire\n",
        "sample), which is then concatenated with a vector of class prevalence estimates produced by an ensemble of simpler\n",
        "quantification methods (CC, ACC, PCC, PACC, SLD). This vector is then transformed by a set of feed-forward layers,\n",
        "followed by ReLU activations and dropout, to compute the final estimations.`\n",
        "\n",
        "QuaNet thus requires a probabilistic classifier that can provide embedded representations of the inputs. QuaPy offers\n",
        "a basic implementation of such a classifier, based on convolutional neural networks, that returns its next-to-last representation as the document embedding. The following is a working example showing how to index a textual dataset\n",
        "(see Section 3) and how to instantiate QuaNet:\n",
        "```python\n",
        "1 import quapy as qp\n",
        "2 from quapy.method.meta import QuaNet\n",
        "3 from classification.neural import NeuralClassifierTrainer, CNNnet\n",
        "4\n",
        "5 qp.environ['SAMPLE_SIZE'] = 500\n",
        "6\n",
        "7 # load the kindle dataset as plain text, and convert words\n",
        "8 # to numerical indexes\n",
        "9 dataset = qp.datasets.fetch_reviews('kindle', pickle=True)\n",
        "10 qp.data.preprocessing.index(dataset, min_df=5, inplace=True)\n",
        "11\n",
        "12 cnn = CNNnet(dataset.vocabulary_size, dataset.n_classes)\n",
        "13 learner = NeuralClassifierTrainer(cnn, device='cuda')\n",
        "14 model = QuaNet(learner, sample_size=500, device='cuda')\n",
        "```\n",
        "QuaNet paper link : Esuli, A., Moreo, A., & Sebastiani, F. (2018, October). A recurrent neural network for sentiment quantification. In Proceedings of the 27th ACM International Conference on Information and Knowledge Management (pp. 1775-1778).\n",
        "# IMPORTANTE per Preprocessing\n",
        "1. Vedi quale Word Embedding utilizzare\n",
        "2. Vedi se e come utilizzare Bert per generare degli abstract embeddings\n",
        "3. Vedi se ha senso usare altri tipi, come SciBert (che e' stato allenato su papers scientifici)\n",
        "\n",
        "\n",
        "PROBLEMA IMPORTANTISSIMO"
      ],
      "metadata": {
        "id": "l6VgU4LzLQja"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers pytorch-crf==0.7.2 torchmetrics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RJJQkHOUOnuy",
        "outputId": "5c16c923-06ba-4e3e-f17d-8158bf0d36f3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Collecting pytorch-crf==0.7.2\n",
            "  Downloading pytorch_crf-0.7.2-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-1.5.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (2.4.0+cpu)\n",
            "Collecting lightning-utilities>=0.8.0 (from torchmetrics)\n",
            "  Downloading lightning_utilities-0.11.8-py3-none-any.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.9.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (75.1.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.4.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.1.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->torchmetrics) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->torchmetrics) (1.3.0)\n",
            "Downloading pytorch_crf-0.7.2-py3-none-any.whl (9.5 kB)\n",
            "Downloading torchmetrics-1.5.0-py3-none-any.whl (890 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m890.5/890.5 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.11.8-py3-none-any.whl (26 kB)\n",
            "Installing collected packages: pytorch-crf, lightning-utilities, torchmetrics\n",
            "Successfully installed lightning-utilities-0.11.8 pytorch-crf-0.7.2 torchmetrics-1.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import sys\n",
        "import os\n",
        "import collections\n",
        "import random\n",
        "from tqdm import trange,tqdm\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import TensorDataset, DataLoader, Subset\n",
        "from transformers import BasicTokenizer, BertPreTrainedModel, BertModel, BertConfig,  BertTokenizer, get_linear_schedule_with_warmup\n",
        "from torch.optim import AdamW\n",
        "import numpy as np\n",
        "from torch import nn\n",
        "from torchcrf import CRF\n",
        "import torchmetrics"
      ],
      "metadata": {
        "id": "Gsc2QAm5S41K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9e2f489-42be-4798-d739-9feae9ac7248"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch_xla/__init__.py:202: UserWarning: `tensorflow` can conflict with `torch-xla`. Prefer `tensorflow-cpu` when using PyTorch/XLA. To silence this warning, `pip uninstall -y tensorflow && pip install tensorflow-cpu`. If you are in a notebook environment such as Colab or Kaggle, restart your notebook runtime afterwards.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ok7lf8KG7KgF",
        "outputId": "5cdab164-9fef-4399-f3be-2c77e822d5c2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = '/content/drive/My Drive/NLPProjectWork/data/neoplasm'\n",
        "# Ensure the directory exists\n",
        "if os.path.exists(data_dir):\n",
        "    os.chdir(data_dir)\n",
        "    print(f\"Current directory: {os.getcwd()}\")\n",
        "else:\n",
        "    print(\"Directory not found.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9u-buDKd7PGe",
        "outputId": "4a51539b-c02a-4cfc-c7b9-a56ead599f8a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current directory: /content/drive/My Drive/NLPProjectWork/data/neoplasm\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def set_seed(seed, n_gpu):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if n_gpu > 0:\n",
        "        torch.cuda.manual_seed_all(seed)"
      ],
      "metadata": {
        "id": "HntvYy00AcdG"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_gpu = torch.cuda.device_count()\n",
        "set_seed(seed = 2, n_gpu = n_gpu)"
      ],
      "metadata": {
        "id": "Pk7HO6PT3zmK"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PREPROCESSING\n"
      ],
      "metadata": {
        "id": "EcivXELPorgq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class InputExample(object):\n",
        "    def __init__(self, guid, text, labels=None):\n",
        "        self.guid = guid\n",
        "        self.text = text\n",
        "        self.labels = labels\n",
        "\n",
        "\n",
        "class InputFeatures(object):\n",
        "    def __init__(self, input_ids, input_mask, segment_ids, label_ids, label_proba):\n",
        "        self.input_ids = input_ids\n",
        "        self.input_mask = input_mask\n",
        "        self.segment_ids = segment_ids\n",
        "        self.label_ids = label_ids\n",
        "        self.label_proba = label_proba"
      ],
      "metadata": {
        "id": "LZz7OS-1os0V"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DataProcessor(object):\n",
        "    def __init__(self):\n",
        "        self.labels = [\"X\", \"B-Claim\", \"I-Claim\", \"B-Premise\", \"I-Premise\", 'O']\n",
        "        self.label_map = self._create_label_map()\n",
        "        self.replace_labels = {\n",
        "            'B-MajorClaim': 'B-Claim',\n",
        "            'I-MajorClaim': 'I-Claim',\n",
        "        }\n",
        "\n",
        "    def _create_label_map(self):\n",
        "        label_map = collections.OrderedDict()\n",
        "        for i, label in enumerate(self.labels):\n",
        "            label_map[label] = i\n",
        "        return label_map\n",
        "\n",
        "    def get_train_examples(self, data_dir):\n",
        "        \"\"\"See base class.\"\"\"\n",
        "        return self._create_examples(\n",
        "            self.read_conll(os.path.join(data_dir, \"train.conll\"), replace=self.replace_labels), \"train\")\n",
        "\n",
        "    def get_dev_examples(self, data_dir):\n",
        "        \"\"\"See base class.\"\"\"\n",
        "        return self._create_examples(\n",
        "            self.read_conll(os.path.join(data_dir, \"dev.conll\"), replace=self.replace_labels), \"dev\")\n",
        "\n",
        "    def get_test_examples(self, data_dir, setname=\"test.conll\"):\n",
        "        \"\"\"See base class.\"\"\"\n",
        "        return self._create_examples(\n",
        "            self.read_conll(os.path.join(data_dir, setname), replace=self.replace_labels), \"test\")\n",
        "\n",
        "    def get_labels(self):\n",
        "        \"\"\" See base class.\"\"\"\n",
        "        return self.labels\n",
        "\n",
        "    def convert_labels_to_ids(self, labels):\n",
        "        idx_list = []\n",
        "        for label in labels:\n",
        "            idx_list.append(self.label_map[label])\n",
        "        return idx_list\n",
        "\n",
        "    def convert_ids_to_labels(self, idx_list):\n",
        "        labels_list = []\n",
        "        for idx in idx_list:\n",
        "            labels_list.append([key for key in self.label_map.keys() if self.label_map[key] == idx][0])\n",
        "        return labels_list\n",
        "\n",
        "    def _create_examples(self, lines, set_type):\n",
        "        \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
        "        examples = []\n",
        "        for (i, line) in enumerate(lines):\n",
        "            guid = \"%s-%s\" % (set_type, str(i))\n",
        "            text = line[0]\n",
        "            labels = line[-1]\n",
        "            examples.append(\n",
        "                InputExample(guid=guid, text=text, labels=labels))\n",
        "        return examples\n",
        "\n",
        "    def convert_examples_to_features(self, examples, max_seq_length, tokenizer,\n",
        "                                     cls_token='[CLS]',\n",
        "                                     sep_token='[SEP]'):\n",
        "        \"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"\n",
        "\n",
        "        features = []\n",
        "        for (ex_index, example) in enumerate(examples):\n",
        "            tokens_a, labels = tokenizer.tokenize_with_label_extension(example.text, example.labels,\n",
        "                                                                       copy_previous_label=True)\n",
        "\n",
        "            # Account for [CLS] and [SEP] with \"- 2\"\n",
        "            if len(tokens_a) > max_seq_length - 2:\n",
        "                tokens_a = tokens_a[:(max_seq_length - 2)]\n",
        "                labels = labels[:(max_seq_length - 2)]\n",
        "            labels = [\"X\"] + labels + [\"X\"]\n",
        "\n",
        "            tokens = [cls_token] + tokens_a + [sep_token]\n",
        "            segment_ids = [0] * len(tokens)\n",
        "            input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "            # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
        "            # tokens are attended to.\n",
        "            input_mask = [1] * len(input_ids)\n",
        "            # Zero-pad up to the sequence length.\n",
        "            padding = [0] * (max_seq_length - len(input_ids))\n",
        "            input_ids += padding\n",
        "            input_mask += padding\n",
        "            segment_ids += padding\n",
        "            label_ids = self.convert_labels_to_ids(labels)\n",
        "            # Se il mask e' None allora non fare nulla\n",
        "            # Altrimenti maska sta label\n",
        "            label_proba = self.compute_label_probadist(label_ids)\n",
        "            # leva il primo e l'ultimo elemento\n",
        "            # assert(non contiene 0 all'interno)\n",
        "            # nel caso delle predizioni se contiene 0 che devo fare? Direi di skippare tutti gli zeri dal calcolo della proba di dist\n",
        "\n",
        "            label_ids += padding\n",
        "\n",
        "            assert len(label_ids) == max_seq_length\n",
        "            assert len(input_ids) == max_seq_length\n",
        "            assert len(input_mask) == max_seq_length\n",
        "            assert len(segment_ids) == max_seq_length\n",
        "\n",
        "            features.append(\n",
        "                InputFeatures(input_ids=input_ids,\n",
        "                              input_mask=input_mask,\n",
        "                              segment_ids=segment_ids,\n",
        "                              label_ids=label_ids,\n",
        "                              label_proba=label_proba))\n",
        "        return features\n",
        "\n",
        "    def compute_label_probadist(self, label_ids, mask=None):\n",
        "        label_ids_np = np.array(label_ids)\n",
        "        if mask is not None:\n",
        "            mask_np = mask.cpu().detach().numpy()\n",
        "            label_ids_np = label_ids_np[mask_np == 1]\n",
        "        label_ids_np = label_ids_np[1:-1]\n",
        "        label_ids_np = label_ids_np[label_ids_np != 0]\n",
        "        if len(label_ids_np) == 0:\n",
        "            return [0.0, 0.0, 0.0]\n",
        "        assert (0 not in label_ids_np)\n",
        "        map = {self.label_map[\"B-Claim\"]: 0,\n",
        "               self.label_map[\"I-Claim\"]: 0,\n",
        "               self.label_map[\"B-Premise\"]: 1,\n",
        "               self.label_map[\"I-Premise\"]: 1,\n",
        "               self.label_map[\"O\"]: 2}\n",
        "        remapped_label_ids = [map[label] for label in label_ids_np]\n",
        "        counts = np.bincount(remapped_label_ids, minlength=3)\n",
        "        probabilities = counts / len(remapped_label_ids)\n",
        "        return probabilities\n",
        "\n",
        "    @classmethod\n",
        "    def features_to_dataset(cls, feature_list):\n",
        "        all_input_ids = torch.tensor([f.input_ids for f in feature_list], dtype=torch.long)\n",
        "        all_input_mask = torch.tensor([f.input_mask for f in feature_list], dtype=torch.long)\n",
        "        all_segment_ids = torch.tensor([f.segment_ids for f in feature_list], dtype=torch.long)\n",
        "        all_label_ids = torch.tensor([f.label_ids for f in feature_list], dtype=torch.long)\n",
        "        all_label_probas = torch.tensor([np.array(f.label_proba) for f in feature_list])\n",
        "        dataset = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids, all_label_probas)\n",
        "        return dataset\n",
        "\n",
        "    @classmethod\n",
        "    def read_conll(cls, input_file, token_number=0, token_column=1, label_column=4, replace=None):\n",
        "        \"\"\"Reads a conll type file.\"\"\"\n",
        "        with open(input_file, \"r\", encoding='utf-8') as f:\n",
        "            lines = f.readlines()\n",
        "            sentences = []\n",
        "            tokenizer = BasicTokenizer()\n",
        "            sent_tokens = []\n",
        "            sent_labels = []\n",
        "\n",
        "            for idx, line in enumerate(lines):\n",
        "\n",
        "                line = line.split('\\t')\n",
        "                # Skip the lines in which there is /n\n",
        "                if len(line) < 2:\n",
        "                    continue\n",
        "\n",
        "                # Controllare se sono all'ID numero 1, assicurandomi nel mentre che NON sia la prima iterazione\n",
        "                if idx != 0 and int(line[token_number]) == 0:\n",
        "                    assert len(sent_tokens) == len(sent_labels)\n",
        "                    if replace:\n",
        "                        sent_labels = [replace[label] if label in replace.keys() else label for label in sent_labels]\n",
        "                    sentences.append([' '.join(sent_tokens), sent_labels])\n",
        "                    sent_tokens = []\n",
        "                    sent_labels = []\n",
        "\n",
        "                token = line[token_column]\n",
        "                label = line[label_column].replace('\\n', '')\n",
        "                tokenized = tokenizer.tokenize(token)\n",
        "\n",
        "                if len(tokenized) > 1:\n",
        "                    for i in range(len(tokenized)):\n",
        "                        sent_tokens.append(tokenized[i])\n",
        "                        sent_labels.append(label)\n",
        "                else:\n",
        "                    sent_tokens.append(tokenized[0])\n",
        "                    sent_labels.append(label)\n",
        "            if sent_tokens != []:\n",
        "                assert len(sent_tokens) == len(sent_labels)\n",
        "                if replace:\n",
        "                    sent_labels = [replace[label] if label in replace.keys() else label for label in sent_labels]\n",
        "                sentences.append([' '.join(sent_tokens), sent_labels])\n",
        "        return sentences\n",
        "\n",
        "    def load_examples(self, data_dir, max_seq_length, tokenizer, evaluate=False, isval=False):\n",
        "        if evaluate:\n",
        "            examples = self.get_test_examples(data_dir)\n",
        "        elif isval:\n",
        "            examples = self.get_dev_examples(data_dir)\n",
        "        else:\n",
        "            examples = self.get_train_examples(data_dir)\n",
        "        features = self.convert_examples_to_features(examples, max_seq_length=max_seq_length, tokenizer=tokenizer)\n",
        "        dataset = self.features_to_dataset(features)\n",
        "        return dataset"
      ],
      "metadata": {
        "id": "xDVe4NSupIhP"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ExtendedBertTokenizer:\n",
        "    \"\"\"Extended tokenizer that wraps a base tokenizer.\"\"\"\n",
        "\n",
        "    def __init__(self, base_tokenizer):\n",
        "        self.tokenizer = base_tokenizer\n",
        "\n",
        "    def tokenize_with_label_extension(self, text, labels, copy_previous_label=False, extension_label='X'):\n",
        "        tok_text = self.tokenizer.tokenize(text)\n",
        "        for i in range(len(tok_text)):\n",
        "            if '##' in tok_text[i]:\n",
        "                if copy_previous_label:\n",
        "                    labels.insert(i, labels[i - 1])\n",
        "                else:\n",
        "                    labels.insert(i, extension_label)\n",
        "        return tok_text, labels\n",
        "\n",
        "    def convert_tokens_to_ids(self, tokens):\n",
        "      return self.tokenizer.convert_tokens_to_ids(tokens)"
      ],
      "metadata": {
        "id": "2xMC_rX_pQVv"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MODELS"
      ],
      "metadata": {
        "id": "ZR9Uz7YFog2Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BertForSequenceTagging(BertPreTrainedModel):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.num_labels = config.num_labels\n",
        "        self.bert = BertModel(config)\n",
        "        self.rnn = nn.GRU(config.hidden_size, config.hidden_size, batch_first=True, bidirectional=True)\n",
        "        self.crf = CRF(config.num_labels, batch_first=True)\n",
        "        self.classifier = nn.Linear(2 * config.hidden_size, config.num_labels)\n",
        "        self.custom_init_weights()\n",
        "        self.name = \"seq_tag_model\"\n",
        "\n",
        "    def custom_init_weights(self):\n",
        "        # Initialize weights of GRU\n",
        "        for name, param in self.rnn.named_parameters():\n",
        "            if 'weight' in name:\n",
        "                nn.init.xavier_uniform_(param, gain=0.01)\n",
        "            elif 'bias' in name:\n",
        "                nn.init.zeros_(param)  # Initialize biases to zero\n",
        "\n",
        "        # Initialize weights of CRF transitions\n",
        "        nn.init.normal_(self.crf.start_transitions, mean=0, std=0.1)\n",
        "        nn.init.normal_(self.crf.end_transitions, mean=0, std=0.1)\n",
        "        nn.init.normal_(self.crf.transitions, mean=0, std=0.1)\n",
        "\n",
        "        # Initialize weights of the classifier\n",
        "        nn.init.xavier_uniform_(self.classifier.weight)\n",
        "        if self.classifier.bias is not None:\n",
        "            nn.init.zeros_(self.classifier.bias)\n",
        "    def forward(\n",
        "            self,\n",
        "            input_ids=None,\n",
        "            attention_mask=None,\n",
        "            token_type_ids=None,\n",
        "            labels=None,\n",
        "            labels_proba=None\n",
        "    ):\n",
        "\n",
        "        outputs = self.bert(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids\n",
        "        )\n",
        "        sequence_output = outputs[0]\n",
        "\n",
        "        print(f\"Sequence output (BERT): {sequence_output.min()}, {sequence_output.max()}\")\n",
        "        rnn_out, _ = self.rnn(sequence_output)\n",
        "        emissions = self.classifier(rnn_out)\n",
        "\n",
        "        mask = attention_mask.bool()\n",
        "        log_likelihood = self.crf(emissions, labels, mask=mask)\n",
        "        path = self.crf.decode(emissions)\n",
        "        path = torch.LongTensor(path)\n",
        "        return (-1 * log_likelihood, emissions, path, None)"
      ],
      "metadata": {
        "id": "lKVDY9qTocBy"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ClassificationAndCounting:\n",
        "    def __init__(self, learner, processor: DataProcessor):\n",
        "        self.learner = learner\n",
        "        self.processor = processor\n",
        "        self.name = \"bert_classify_and_count\"\n",
        "\n",
        "    def forward(\n",
        "            self,\n",
        "            input_ids=None,\n",
        "            attention_mask=None,\n",
        "            token_type_ids=None,\n",
        "            labels=None,\n",
        "            labels_proba=None\n",
        "    ):\n",
        "\n",
        "        loss, emissions, path, _ = self.learner(input_ids, attention_mask, token_type_ids, labels, labels_proba)\n",
        "\n",
        "        proba_dists = self.aggregate(path, attention_mask)\n",
        "        return loss, emissions, path, proba_dists\n",
        "\n",
        "    def aggregate(self, predictions, attention_mask):\n",
        "        all_probabilities = []\n",
        "        for sequence, mask in zip(predictions, attention_mask):\n",
        "            label_proba = self.processor.compute_label_probadist(sequence, mask=mask)\n",
        "            all_probabilities.append(label_proba)\n",
        "        return np.array(all_probabilities)\n",
        "\n",
        "    # Wrapper methods mimicking PyTorch model behavior\n",
        "    def train(self, mode=True):\n",
        "        self.learner.train(mode)\n",
        "\n",
        "    def eval(self):\n",
        "        self.learner.eval()\n",
        "\n",
        "    def state_dict(self, *args, **kwargs):\n",
        "        return self.learner.state_dict(*args, **kwargs)\n",
        "\n",
        "    def load_state_dict(self, *args, **kwargs):\n",
        "        return self.learner.load_state_dict(*args, **kwargs)\n",
        "\n",
        "    def to(self, *args, **kwargs):\n",
        "        self.learner.to(*args, **kwargs)\n",
        "\n",
        "    def parameters(self):\n",
        "        return self.learner.parameters()\n",
        "\n",
        "    def named_parameters(self):\n",
        "        return self.learner.named_parameters()\n",
        "\n",
        "    def zero_grad(self):\n",
        "        self.learner.zero_grad()\n",
        "\n",
        "    def __call__(self, *args, **kwargs):\n",
        "        return self.forward(*args, **kwargs)"
      ],
      "metadata": {
        "id": "2cbm6HJwoU1P"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BertForLabelDistribution(BertPreTrainedModel):\n",
        "    def __init__(self, config, loss_fn, loss_fn_name):\n",
        "        super().__init__(config)\n",
        "        self.num_labels = config.num_labels\n",
        "        self.bert = BertModel(config)\n",
        "        self.rnn = nn.GRU(config.hidden_size, config.hidden_size, batch_first=True, bidirectional=True)\n",
        "        self.classifier = nn.Linear(2 * config.hidden_size, config.num_labels)\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "        self.init_weights()\n",
        "        self.loss_fn = loss_fn\n",
        "        self.name = f\"bert_quantify_{loss_fn_name}\"\n",
        "        self.custom_init_weights()\n",
        "\n",
        "    def custom_init_weights(self):\n",
        "        # Initialize weights of GRU\n",
        "        for name, param in self.rnn.named_parameters():\n",
        "            if 'weight' in name:\n",
        "                nn.init.xavier_uniform_(param, gain=0.01)\n",
        "            elif 'bias' in name:\n",
        "                nn.init.zeros_(param)  # Initialize biases to zero\n",
        "\n",
        "        # Initialize weights of the classifier\n",
        "        nn.init.xavier_uniform_(self.classifier.weight)\n",
        "        if self.classifier.bias is not None:\n",
        "            nn.init.zeros_(self.classifier.bias)\n",
        "\n",
        "    def forward(\n",
        "            self,\n",
        "            input_ids=None,\n",
        "            attention_mask=None,\n",
        "            token_type_ids=None,\n",
        "            labels=None,\n",
        "            labels_proba=None,\n",
        "    ):\n",
        "        outputs = self.bert(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids\n",
        "        )\n",
        "        sequence_output = outputs[0]  # [batch_size, seq_len, hidden_size]\n",
        "        rnn_out, _ = self.rnn(sequence_output)  # [batch_size, seq_len, 2*hidden_size]\n",
        "        emissions = self.classifier(rnn_out)  # [batch_size, seq_len, num_labels]\n",
        "        cls_emissions = emissions[:, 0, :]  # [batch_size, num_labels]\n",
        "        # posos calcolare la loss e dopo la softmax\n",
        "        cls_probs = self.softmax(cls_emissions)  # [batch_size, num_labels]\n",
        "        loss = self.loss_fn(cls_probs, labels_proba)\n",
        "        return loss, emissions, None, cls_probs"
      ],
      "metadata": {
        "id": "dm_FPtFyocz_"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TRAIN/EVALUATE"
      ],
      "metadata": {
        "id": "bxO6U0y7okfe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_quantization_preds_labels(inputs, outputs):\n",
        "    _, _, _, pred_proba = outputs\n",
        "    pred_proba = torch.tensor(pred_proba)\n",
        "    labels_proba = inputs[\"labels_proba\"]\n",
        "    return torch.tensor(pred_proba).to(device),labels_proba\n",
        "\n",
        "def get_classf_preds_labels(inputs, outputs):\n",
        "    loss, emissions, path, _ = outputs\n",
        "    batch_logits = path.detach().cpu().numpy().flatten()\n",
        "    batch_labels = inputs[\"labels\"].detach().cpu().numpy().flatten()\n",
        "    attention_mask = inputs[\"attention_mask\"].detach().cpu().numpy().flatten()\n",
        "    valid_batch_logits = batch_logits[attention_mask == 1]\n",
        "    valid_batch_labels = batch_labels[attention_mask == 1]\n",
        "    return torch.tensor(valid_batch_logits), torch.tensor(valid_batch_labels)"
      ],
      "metadata": {
        "id": "ax6nmcdksLb5"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mae_loss(pred_probas, label_probas):\n",
        "    loss = nn.L1Loss(reduction=\"mean\")\n",
        "    return loss(pred_probas, label_probas)\n",
        "\n",
        "def kldiv_loss(pred_probas, label_probas):\n",
        "    kl_loss = nn.KLDivLoss(reduction=\"batchmean\")\n",
        "    pred_probas_log = torch.log(pred_probas)\n",
        "    output = kl_loss(pred_probas_log, label_probas)\n",
        "    return output"
      ],
      "metadata": {
        "id": "f7Wrj51vqmDC"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_batch(batch, device, model, gen_preds_labels_fn, eval=False):\n",
        "    model.eval() if eval else model.train()\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    inputs = {\"input_ids\": batch[0], \"attention_mask\": batch[1], \"token_type_ids\": batch[2], \"labels\": batch[3],\n",
        "              \"labels_proba\": batch[4]}\n",
        "    with torch.no_grad() if eval else torch.enable_grad():\n",
        "        outputs = model(**inputs)\n",
        "    preds, labels = gen_preds_labels_fn(inputs, outputs)\n",
        "    return outputs, preds, labels"
      ],
      "metadata": {
        "id": "yPgB9VI1nkns"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(device, model, eval_dataset, eval_batch_size, metrics, gen_preds_labels_fn):\n",
        "    eval_dataloader = DataLoader(eval_dataset, batch_size=eval_batch_size)\n",
        "    epoch_loss_sum = 0.0\n",
        "    for metric in metrics:\n",
        "        metric.reset()\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
        "            outputs, preds, labels = process_batch(batch, device, model, gen_preds_labels_fn, eval=True)\n",
        "            loss = outputs[0]\n",
        "            epoch_loss_sum += loss.item()\n",
        "            for metric in metrics:\n",
        "                if metric.__class__.__name__ == \"KLDivergence\":\n",
        "                      labels = labels.clamp(min=1e-9)\n",
        "                      preds = preds.clamp(min=1e-9)\n",
        "                metric.update(preds, labels)\n",
        "    eval_loss = epoch_loss_sum / len(eval_dataloader)\n",
        "    eval_metrics = {metric.__class__.__name__: metric.compute().item() for metric in metrics}\n",
        "    return eval_loss, eval_metrics"
      ],
      "metadata": {
        "id": "QOC4shMInjVE"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def update_metrics(history, metrics, metric_type, epoch):\n",
        "    for metric_name, metric_value in metrics.items():\n",
        "        if epoch == 0:\n",
        "            history[metric_type][metric_name] = [metric_value]\n",
        "        else:\n",
        "            history[metric_type][metric_name].append(metric_value)\n",
        "        print(f\"{metric_name}: {metric_value:.4f}\")\n",
        "def train(\n",
        "        device,\n",
        "        train_dataset,\n",
        "        model,\n",
        "        eval_dataset,\n",
        "        generate_preds_labels_fn,\n",
        "        metrics,\n",
        "        num_train_epochs=30,\n",
        "        train_batch_size=5,\n",
        "        eval_batch_size=32,\n",
        "        weight_decay=0.3,\n",
        "        learning_rate=2e-5,\n",
        "        adam_epsilon=1e-8,\n",
        "        warmup_steps=5,\n",
        "        max_grad_norm = 3.0\n",
        "):\n",
        "    history = {\n",
        "        'train_loss': [],\n",
        "        'train_metrics': {},\n",
        "        'eval_loss': [],\n",
        "        'eval_metrics': {}\n",
        "    }\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=train_batch_size)\n",
        "    num_steps_per_epoch = len(train_dataloader)\n",
        "    t_total = num_steps_per_epoch * num_train_epochs\n",
        "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
        "    optimizer_grouped_parameters = [\n",
        "        {\n",
        "            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
        "            \"weight_decay\": weight_decay,\n",
        "        },\n",
        "        {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n",
        "    ]\n",
        "    optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate, eps=adam_epsilon)\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer, num_warmup_steps=warmup_steps, num_training_steps=t_total\n",
        "    )\n",
        "    epochs_trained = 0\n",
        "\n",
        "    train_iterator = trange(epochs_trained, int(num_train_epochs), desc=\"Epoch\")\n",
        "    print(f\"TRAINING MODEL {model.name}\")\n",
        "    for epoch in train_iterator:\n",
        "        epoch_loss_sum = 0.0\n",
        "        epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\")\n",
        "        for step, batch in enumerate(epoch_iterator):\n",
        "            optimizer.zero_grad()\n",
        "            outputs, preds, labels = process_batch(batch, device, model, generate_preds_labels_fn, eval=False)\n",
        "            loss = outputs[0]\n",
        "            print(f\"SINGLE LOSS: {loss}\")\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "            epoch_loss_sum += loss.item()\n",
        "            for metric in metrics:\n",
        "                if metric.__class__.__name__ == \"KLDivergence\":\n",
        "                      labels = labels.clamp(min=1e-9)\n",
        "                      preds = preds.clamp(min=1e-9)\n",
        "                metric.update(preds, labels)\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "        print(f\"CUMULATIVE LOSSES: {epoch_loss_sum}\")\n",
        "        epoch_loss = epoch_loss_sum / num_steps_per_epoch\n",
        "        history['train_loss'].append(epoch_loss)\n",
        "        epoch_metrics = {metric.__class__.__name__: metric.compute().item() for metric in metrics}\n",
        "        eval_loss, eval_metrics = evaluate(device, model, eval_dataset, eval_batch_size, metrics,\n",
        "                                           generate_preds_labels_fn)\n",
        "        history['eval_loss'].append(eval_loss)\n",
        "        print(f\"Epoch {epoch + 1}/{num_train_epochs}\")\n",
        "        print(f\"Train Loss: {epoch_loss:.4f}\")\n",
        "        update_metrics(history, epoch_metrics, 'train_metrics', epoch)\n",
        "        print(f\"Eval Loss: {eval_loss:.4f}\")\n",
        "        update_metrics(history, eval_metrics, 'eval_metrics', epoch)\n",
        "    return history"
      ],
      "metadata": {
        "id": "MYd-aQQxngu_"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RESULTS"
      ],
      "metadata": {
        "id": "dedHEP35pXxo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name_or_path = 'allenai/scibert_scivocab_uncased'\n",
        "do_lower_case = True\n",
        "base_tokenizer = BertTokenizer.from_pretrained(model_name_or_path, do_lower_case=do_lower_case)\n",
        "extended_tokenizer = ExtendedBertTokenizer(base_tokenizer)"
      ],
      "metadata": {
        "id": "JrLrfYxWVu6d"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "dataprocessor = DataProcessor()\n",
        "max_seq_length = 510"
      ],
      "metadata": {
        "id": "ouaGaD9nV0pf"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = dataprocessor.load_examples(data_dir=data_dir, max_seq_length=max_seq_length,\n",
        "                          tokenizer=extended_tokenizer)\n",
        "val_ds = dataprocessor.load_examples( data_dir=data_dir, max_seq_length=max_seq_length,\n",
        "                        tokenizer=extended_tokenizer, isval=True)\n",
        "test_ds = dataprocessor.load_examples(data_dir=data_dir, max_seq_length=max_seq_length,\n",
        "                        tokenizer=extended_tokenizer, evaluate=True)"
      ],
      "metadata": {
        "id": "RPJE5K_VWAo-"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CREATE CLASSIFICATION+COUNT MODEL"
      ],
      "metadata": {
        "id": "Yc3a-rg6WDhX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "num_class_labels = len(dataprocessor.get_labels())\n",
        "\"\"\"\n",
        "seq_tagging_config = BertConfig.from_pretrained(\n",
        "    model_name_or_path,\n",
        "    num_labels=num_class_labels\n",
        ")\n",
        "seq_tagging_model = BertForSequenceTagging.from_pretrained(\n",
        "    model_name_or_path,\n",
        "    config=seq_tagging_config\n",
        ")\n",
        "multiclass_metrics = [torchmetrics.Accuracy(num_classes=6, average='macro', task = \"multiclass\").to(device), torchmetrics.F1Score(num_classes=6, average='macro', task = \"multiclass\").to(device)]\n",
        "history = train(device=device, train_dataset=train_ds, model=seq_tagging_model, eval_dataset=val_ds, generate_preds_labels_fn=get_classf_preds_labels, metrics=multiclass_metrics)\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "collapsed": true,
        "id": "a1qmzJ8gWLIA",
        "outputId": "f97428d7-2d20-4ecb-da02-db112f26a5d7"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nseq_tagging_config = BertConfig.from_pretrained(\\n    model_name_or_path,\\n    num_labels=num_class_labels\\n)\\nseq_tagging_model = BertForSequenceTagging.from_pretrained(\\n    model_name_or_path,\\n    config=seq_tagging_config\\n)\\nmulticlass_metrics = [torchmetrics.Accuracy(num_classes=6, average=\\'macro\\', task = \"multiclass\").to(device), torchmetrics.F1Score(num_classes=6, average=\\'macro\\', task = \"multiclass\").to(device)]\\nhistory = train(device=device, train_dataset=train_ds, model=seq_tagging_model, eval_dataset=val_ds, generate_preds_labels_fn=get_classf_preds_labels, metrics=multiclass_metrics)\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classify_count_config = BertConfig.from_pretrained(\n",
        "    model_name_or_path,\n",
        "    num_labels=num_class_labels)\n",
        "classify_count_base_model = BertForSequenceTagging.from_pretrained(\n",
        "    model_name_or_path,\n",
        "    config=classify_count_config\n",
        ")\n",
        "classify_and_count_model = ClassificationAndCounting(learner=classify_count_base_model, processor=dataprocessor)\n",
        "classify_and_count_model.to(device)\n",
        "dist_metrics = [torchmetrics.KLDivergence(log_prob=False, reduction=\"mean\").to(device),\n",
        "                torchmetrics.MeanAbsoluteError().to(device),\n",
        "                torchmetrics.MeanSquaredError().to(device)]\n",
        "history = train(device=device, train_dataset=train_ds, model=classify_and_count_model, eval_dataset=val_ds, generate_preds_labels_fn=get_quantization_preds_labels, metrics=dist_metrics)\n"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ppMVCxKcGSAT",
        "outputId": "70684bc1-ad28-42ee-d921-8a65b47f28c2"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceTagging were not initialized from the model checkpoint at allenai/scibert_scivocab_uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'crf.end_transitions', 'crf.start_transitions', 'crf.transitions', 'rnn.bias_hh_l0', 'rnn.bias_hh_l0_reverse', 'rnn.bias_ih_l0', 'rnn.bias_ih_l0_reverse', 'rnn.weight_hh_l0', 'rnn.weight_hh_l0_reverse', 'rnn.weight_ih_l0', 'rnn.weight_ih_l0_reverse']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Epoch:   0%|          | 0/30 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAINING MODEL bert_classify_and_count\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Iteration:   0%|          | 0/70 [00:00<?, ?it/s]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequence output (BERT): -14.715432167053223, 17.918481826782227\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-01789ea52e69>:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  return torch.tensor(pred_proba).to(device),labels_proba\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SINGLE LOSS: 2.534143844386498e+20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Iteration:   1%|▏         | 1/70 [00:04<05:16,  4.59s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequence output (BERT): -15.671463012695312, 17.03908348083496\n",
            "SINGLE LOSS: 2.534143844386498e+20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Iteration:   3%|▎         | 2/70 [00:11<06:41,  5.91s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequence output (BERT): -15.093567848205566, 17.265661239624023\n",
            "SINGLE LOSS: 2.53414067779301e+20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Iteration:   4%|▍         | 3/70 [00:15<05:30,  4.93s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequence output (BERT): -17.243614196777344, 17.31109619140625\n",
            "SINGLE LOSS: 2.5341348723716155e+20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Iteration:   6%|▌         | 4/70 [00:21<06:13,  5.65s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequence output (BERT): -16.56304359436035, 17.92195701599121\n",
            "SINGLE LOSS: 2.534125900356733e+20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Iteration:   7%|▋         | 5/70 [00:27<06:03,  5.59s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequence output (BERT): -14.891651153564453, 17.696239471435547\n",
            "SINGLE LOSS: 2.5341135858265018e+20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Iteration:   9%|▊         | 6/70 [00:33<05:57,  5.59s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequence output (BERT): -17.54294204711914, 16.878068923950195\n",
            "SINGLE LOSS: 2.534098280624643e+20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Iteration:  10%|█         | 7/70 [00:36<05:04,  4.83s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequence output (BERT): -17.714153289794922, 17.510231018066406\n",
            "SINGLE LOSS: 2.534083151344645e+20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Iteration:  11%|█▏        | 8/70 [00:40<04:51,  4.70s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequence output (BERT): -17.389965057373047, 17.783634185791016\n",
            "SINGLE LOSS: 2.5340680220646467e+20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Iteration:  13%|█▎        | 9/70 [00:48<05:36,  5.52s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequence output (BERT): -17.308269500732422, 17.728979110717773\n",
            "SINGLE LOSS: 2.534052716862788e+20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Iteration:  14%|█▍        | 10/70 [00:56<06:17,  6.29s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequence output (BERT): -18.500185012817383, 17.827577590942383\n",
            "SINGLE LOSS: 2.53403758758279e+20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Iteration:  16%|█▌        | 11/70 [01:00<05:38,  5.73s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequence output (BERT): -17.943553924560547, 17.277103424072266\n",
            "SINGLE LOSS: 2.5340224583027917e+20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Iteration:  17%|█▋        | 12/70 [01:04<04:59,  5.17s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequence output (BERT): -17.4008846282959, 17.08854103088379\n",
            "SINGLE LOSS: 2.5340073290227935e+20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Iteration:  19%|█▊        | 13/70 [01:07<04:20,  4.57s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequence output (BERT): -17.47627830505371, 17.1535701751709\n",
            "SINGLE LOSS: 2.5339921997427953e+20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Iteration:  20%|██        | 14/70 [01:10<03:48,  4.08s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequence output (BERT): -17.040822982788086, 17.19114875793457\n",
            "SINGLE LOSS: 2.5339768945409366e+20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Iteration:  21%|██▏       | 15/70 [01:13<03:22,  3.69s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequence output (BERT): -15.144891738891602, 16.745859146118164\n",
            "SINGLE LOSS: 2.5339617652609384e+20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Iteration:  23%|██▎       | 16/70 [01:17<03:24,  3.79s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequence output (BERT): -17.739423751831055, 16.314838409423828\n",
            "SINGLE LOSS: 2.5339466359809402e+20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Iteration:  24%|██▍       | 17/70 [01:20<03:12,  3.64s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequence output (BERT): -16.806970596313477, 17.811214447021484\n",
            "SINGLE LOSS: 2.5339313307790816e+20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Iteration:  26%|██▌       | 18/70 [01:23<03:02,  3.52s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequence output (BERT): -16.530794143676758, 16.860170364379883\n",
            "SINGLE LOSS: 2.5339162014990834e+20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Iteration:  27%|██▋       | 19/70 [01:26<02:51,  3.36s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequence output (BERT): -14.891195297241211, 16.54011344909668\n",
            "SINGLE LOSS: 2.5339010722190852e+20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Iteration:  29%|██▊       | 20/70 [01:29<02:41,  3.24s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequence output (BERT): -17.376096725463867, 16.689987182617188\n",
            "SINGLE LOSS: 2.533885942939087e+20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Iteration:  30%|███       | 21/70 [01:33<02:40,  3.27s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequence output (BERT): -16.15793800354004, 16.612966537475586\n",
            "SINGLE LOSS: 2.5338708136590888e+20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Iteration:  31%|███▏      | 22/70 [01:39<03:15,  4.07s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequence output (BERT): -17.561309814453125, 16.444318771362305\n",
            "SINGLE LOSS: 2.5338555084572302e+20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Iteration:  33%|███▎      | 23/70 [01:42<02:55,  3.73s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequence output (BERT): -17.671537399291992, 16.71509552001953\n",
            "SINGLE LOSS: 2.533840379177232e+20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Iteration:  34%|███▍      | 24/70 [01:44<02:40,  3.49s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequence output (BERT): -17.29938507080078, 16.063871383666992\n",
            "SINGLE LOSS: 2.5338252498972338e+20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Iteration:  36%|███▌      | 25/70 [01:48<02:33,  3.42s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequence output (BERT): -16.938488006591797, 15.96292495727539\n",
            "SINGLE LOSS: 2.533809944695375e+20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Iteration:  37%|███▋      | 26/70 [01:55<03:24,  4.65s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequence output (BERT): -15.730973243713379, 16.14704704284668\n",
            "SINGLE LOSS: 2.533794815415377e+20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Iteration:  39%|███▊      | 27/70 [01:58<02:58,  4.16s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequence output (BERT): -17.444814682006836, 15.888656616210938\n",
            "SINGLE LOSS: 2.5337796861353787e+20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Iteration:  40%|████      | 28/70 [02:01<02:41,  3.84s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequence output (BERT): -16.692922592163086, 16.07956886291504\n",
            "SINGLE LOSS: 2.5337645568553805e+20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Iteration:  41%|████▏     | 29/70 [02:04<02:29,  3.64s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequence output (BERT): -17.14959716796875, 15.720462799072266\n",
            "SINGLE LOSS: 2.5337494275753823e+20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Iteration:  43%|████▎     | 30/70 [02:08<02:18,  3.46s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequence output (BERT): -17.30865478515625, 15.541646957397461\n",
            "SINGLE LOSS: 2.5337341223735237e+20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Iteration:  44%|████▍     | 31/70 [02:11<02:09,  3.31s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequence output (BERT): -16.76171112060547, 16.06256103515625\n",
            "SINGLE LOSS: 2.533719169015386e+20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Iteration:  46%|████▌     | 32/70 [02:13<02:00,  3.18s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequence output (BERT): -17.56482696533203, 16.0620174407959\n",
            "SINGLE LOSS: 2.5337042156572482e+20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Iteration:  47%|████▋     | 33/70 [02:17<02:00,  3.26s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequence output (BERT): -17.183528900146484, 15.909631729125977\n",
            "SINGLE LOSS: 2.5336892622991104e+20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Iteration:  49%|████▊     | 34/70 [02:20<01:58,  3.29s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequence output (BERT): -16.65809440612793, 15.411611557006836\n",
            "SINGLE LOSS: 2.5336743089409727e+20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Iteration:  50%|█████     | 35/70 [02:23<01:52,  3.22s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequence output (BERT): -17.051801681518555, 15.821745872497559\n",
            "SINGLE LOSS: 2.533659355582835e+20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Iteration:  51%|█████▏    | 36/70 [02:27<01:50,  3.25s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequence output (BERT): -16.95025062561035, 15.576169967651367\n",
            "SINGLE LOSS: 2.5336444022246972e+20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Iteration:  53%|█████▎    | 37/70 [02:31<02:01,  3.70s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequence output (BERT): -14.38640308380127, 15.553511619567871\n",
            "SINGLE LOSS: 2.5336294488665594e+20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Iteration:  54%|█████▍    | 38/70 [02:36<02:04,  3.91s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequence output (BERT): -17.725435256958008, 15.370058059692383\n",
            "SINGLE LOSS: 2.5336144955084217e+20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Iteration:  56%|█████▌    | 39/70 [02:40<02:02,  3.94s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequence output (BERT): -16.452442169189453, 15.581738471984863\n",
            "SINGLE LOSS: 2.533599542150284e+20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Iteration:  57%|█████▋    | 40/70 [02:45<02:07,  4.24s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequence output (BERT): -17.04998016357422, 15.420795440673828\n",
            "SINGLE LOSS: 2.5335845887921462e+20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Iteration:  59%|█████▊    | 41/70 [02:49<02:05,  4.32s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequence output (BERT): -17.233415603637695, 14.919551849365234\n",
            "SINGLE LOSS: 2.5335696354340084e+20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Iteration:  60%|██████    | 42/70 [02:53<01:58,  4.22s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequence output (BERT): -16.576765060424805, 15.161785125732422\n",
            "SINGLE LOSS: 2.5335546820758707e+20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Iteration:  61%|██████▏   | 43/70 [02:57<01:49,  4.04s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequence output (BERT): -17.11048126220703, 14.799696922302246\n",
            "SINGLE LOSS: 2.533539728717733e+20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Iteration:  63%|██████▎   | 44/70 [03:02<01:52,  4.33s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequence output (BERT): -16.872833251953125, 15.398710250854492\n",
            "SINGLE LOSS: 2.533524775359595e+20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Iteration:  64%|██████▍   | 45/70 [03:08<01:59,  4.79s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequence output (BERT): -16.37388801574707, 14.881319999694824\n",
            "SINGLE LOSS: 2.5335098220014574e+20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Iteration:  66%|██████▌   | 46/70 [03:14<02:08,  5.37s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequence output (BERT): -14.512471199035645, 15.207059860229492\n",
            "SINGLE LOSS: 2.5334948686433196e+20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Iteration:  67%|██████▋   | 47/70 [03:19<01:57,  5.09s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequence output (BERT): -13.757569313049316, 14.557049751281738\n",
            "SINGLE LOSS: 2.533479915285182e+20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Iteration:  69%|██████▊   | 48/70 [03:23<01:46,  4.85s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequence output (BERT): -16.855470657348633, 15.287718772888184\n",
            "SINGLE LOSS: 2.533464961927044e+20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Iteration:  70%|███████   | 49/70 [03:28<01:43,  4.94s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequence output (BERT): -15.244176864624023, 14.825758934020996\n",
            "SINGLE LOSS: 2.5334500085689064e+20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Iteration:  71%|███████▏  | 50/70 [03:34<01:40,  5.04s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequence output (BERT): -16.375181198120117, 15.610196113586426\n",
            "SINGLE LOSS: 2.5334350552107686e+20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Iteration:  73%|███████▎  | 51/70 [03:39<01:37,  5.13s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequence output (BERT): -15.788134574890137, 14.931154251098633\n",
            "SINGLE LOSS: 2.533420101852631e+20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Iteration:  74%|███████▍  | 52/70 [03:44<01:31,  5.09s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequence output (BERT): -15.755375862121582, 14.216964721679688\n",
            "SINGLE LOSS: 2.5334053244163536e+20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Iteration:  76%|███████▌  | 53/70 [03:49<01:24,  4.98s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequence output (BERT): -16.79708480834961, 15.536029815673828\n",
            "SINGLE LOSS: 2.5333907229019367e+20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Iteration:  77%|███████▋  | 54/70 [03:54<01:22,  5.14s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequence output (BERT): -15.521397590637207, 14.905611991882324\n",
            "SINGLE LOSS: 2.5333759454656594e+20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Iteration:  79%|███████▊  | 55/70 [03:59<01:16,  5.11s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequence output (BERT): -15.918844223022461, 15.242597579956055\n",
            "SINGLE LOSS: 2.533361168029382e+20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Iteration:  80%|████████  | 56/70 [04:03<01:08,  4.88s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequence output (BERT): -15.063349723815918, 15.274470329284668\n",
            "SINGLE LOSS: 2.5333465665149652e+20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Iteration:  81%|████████▏ | 57/70 [04:09<01:04,  4.99s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequence output (BERT): -15.395509719848633, 14.381698608398438\n",
            "SINGLE LOSS: 2.533331789078688e+20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Iteration:  83%|████████▎ | 58/70 [04:13<00:55,  4.65s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequence output (BERT): -15.066682815551758, 15.208032608032227\n",
            "SINGLE LOSS: 2.5333170116424106e+20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Iteration:  84%|████████▍ | 59/70 [04:16<00:46,  4.18s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequence output (BERT): -16.391067504882812, 14.546611785888672\n",
            "SINGLE LOSS: 2.5333022342061333e+20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Iteration:  86%|████████▌ | 60/70 [04:22<00:47,  4.76s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequence output (BERT): -14.393691062927246, 14.270003318786621\n",
            "SINGLE LOSS: 2.533287456769856e+20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Iteration:  87%|████████▋ | 61/70 [04:29<00:48,  5.37s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequence output (BERT): -14.340975761413574, 14.55958366394043\n",
            "SINGLE LOSS: 2.533272855255439e+20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Iteration:  89%|████████▊ | 62/70 [04:34<00:42,  5.28s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequence output (BERT): -15.71900749206543, 14.878067016601562\n",
            "SINGLE LOSS: 2.5332580778191618e+20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Iteration:  90%|█████████ | 63/70 [04:38<00:35,  5.12s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequence output (BERT): -13.614900588989258, 13.65998649597168\n",
            "SINGLE LOSS: 2.5332433003828845e+20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Iteration:  91%|█████████▏| 64/70 [04:44<00:31,  5.29s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequence output (BERT): -14.201986312866211, 14.102134704589844\n",
            "SINGLE LOSS: 2.5332286988684676e+20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Iteration:  93%|█████████▎| 65/70 [04:50<00:27,  5.43s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequence output (BERT): -12.484776496887207, 13.939964294433594\n",
            "SINGLE LOSS: 2.5332139214321903e+20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Iteration:  94%|█████████▍| 66/70 [04:53<00:19,  4.80s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequence output (BERT): -13.694860458374023, 13.550265312194824\n",
            "SINGLE LOSS: 2.533199143995913e+20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Iteration:  96%|█████████▌| 67/70 [04:56<00:13,  4.34s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequence output (BERT): -12.239469528198242, 12.783123016357422\n",
            "SINGLE LOSS: 2.5331843665596357e+20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Iteration:  97%|█████████▋| 68/70 [05:00<00:08,  4.07s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequence output (BERT): -13.015257835388184, 12.459232330322266\n",
            "SINGLE LOSS: 2.5331695891233584e+20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Iteration:  99%|█████████▊| 69/70 [05:03<00:03,  3.77s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequence output (BERT): -11.853110313415527, 13.234981536865234\n",
            "SINGLE LOSS: 2.5331549876089415e+20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Iteration: 100%|██████████| 70/70 [05:06<00:00,  4.38s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUMULATIVE LOSSES: 1.7735671432231466e+22\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Evaluating:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequence output (BERT): -12.511577606201172, 13.620159149169922\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Evaluating:  50%|█████     | 1/2 [00:05<00:05,  5.92s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequence output (BERT): -12.847799301147461, 13.59028148651123\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Evaluating: 100%|██████████| 2/2 [00:09<00:00,  4.59s/it]\n",
            "Epoch:   3%|▎         | 1/30 [05:15<2:32:38, 315.82s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "Train Loss: 253366734746163806208.0000\n",
            "KLDivergence: 0.1494\n",
            "MeanAbsoluteError: 0.1101\n",
            "MeanSquaredError: 0.0188\n",
            "Eval Loss: 1266570122678518153216.0000\n",
            "KLDivergence: 0.0864\n",
            "MeanAbsoluteError: 0.0861\n",
            "MeanSquaredError: 0.0127\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Iteration:   0%|          | 0/70 [00:00<?, ?it/s]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequence output (BERT): -12.11197566986084, 12.757888793945312\n",
            "SINGLE LOSS: 2.5331402101726642e+20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Iteration:   1%|▏         | 1/70 [00:03<03:46,  3.28s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequence output (BERT): -13.127034187316895, 12.045297622680664\n",
            "SINGLE LOSS: 2.533125432736387e+20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Iteration:   3%|▎         | 2/70 [00:06<03:36,  3.19s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequence output (BERT): -11.412140846252441, 13.758880615234375\n",
            "SINGLE LOSS: 2.53311083122197e+20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Iteration:   4%|▍         | 3/70 [00:09<03:27,  3.10s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequence output (BERT): -11.05139446258545, 12.98026180267334\n",
            "SINGLE LOSS: 2.5330960537856927e+20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Iteration:   6%|▌         | 4/70 [00:12<03:19,  3.02s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequence output (BERT): -11.432028770446777, 13.282960891723633\n",
            "SINGLE LOSS: 2.5330812763494154e+20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Iteration:   7%|▋         | 5/70 [00:15<03:19,  3.06s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequence output (BERT): -9.864572525024414, 13.075700759887695\n",
            "SINGLE LOSS: 2.533066498913138e+20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Iteration:   9%|▊         | 6/70 [00:18<03:17,  3.09s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequence output (BERT): -12.537796020507812, 12.396660804748535\n",
            "SINGLE LOSS: 2.5330517214768608e+20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Iteration:  10%|█         | 7/70 [00:21<03:12,  3.06s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequence output (BERT): -11.982110023498535, 12.428192138671875\n",
            "SINGLE LOSS: 2.533037119962444e+20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Iteration:  11%|█▏        | 8/70 [00:24<03:15,  3.16s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequence output (BERT): -10.837635040283203, 11.934967994689941\n",
            "SINGLE LOSS: 2.5330223425261666e+20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Iteration:  13%|█▎        | 9/70 [00:27<03:07,  3.07s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequence output (BERT): -13.48060417175293, 12.248676300048828\n",
            "SINGLE LOSS: 2.5330075650898893e+20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Iteration:  14%|█▍        | 10/70 [00:30<03:02,  3.05s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequence output (BERT): -11.10561466217041, 12.552366256713867\n",
            "SINGLE LOSS: 2.5329929635754725e+20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Iteration:  16%|█▌        | 11/70 [00:34<03:02,  3.09s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequence output (BERT): -11.248446464538574, 12.917572975158691\n",
            "SINGLE LOSS: 2.532978186139195e+20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Iteration:  17%|█▋        | 12/70 [00:37<02:59,  3.10s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequence output (BERT): -11.750938415527344, 13.190348625183105\n",
            "SINGLE LOSS: 2.532963408702918e+20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Iteration:  19%|█▊        | 13/70 [00:42<03:38,  3.84s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequence output (BERT): -10.665806770324707, 13.824458122253418\n",
            "SINGLE LOSS: 2.5329486312666405e+20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Iteration:  20%|██        | 14/70 [00:45<03:22,  3.62s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequence output (BERT): -10.841449737548828, 13.348066329956055\n",
            "SINGLE LOSS: 2.5329338538303632e+20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Iteration:  21%|██▏       | 15/70 [00:51<03:47,  4.14s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequence output (BERT): -10.234513282775879, 13.251004219055176\n",
            "SINGLE LOSS: 2.5329192523159464e+20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Iteration:  23%|██▎       | 16/70 [00:54<03:24,  3.79s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequence output (BERT): -11.301548957824707, 13.906079292297363\n",
            "SINGLE LOSS: 2.532904474879669e+20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Iteration:  24%|██▍       | 17/70 [00:57<03:08,  3.55s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequence output (BERT): -12.255313873291016, 13.9600248336792\n",
            "SINGLE LOSS: 2.5328896974433917e+20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Iteration:  26%|██▌       | 18/70 [01:00<02:55,  3.37s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequence output (BERT): -11.14191722869873, 13.551706314086914\n",
            "SINGLE LOSS: 2.532875095928975e+20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Iteration:  27%|██▋       | 19/70 [01:03<02:59,  3.52s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequence output (BERT): -12.265870094299316, 13.990854263305664\n",
            "SINGLE LOSS: 2.5328603184926976e+20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Iteration:  29%|██▊       | 20/70 [01:06<02:48,  3.37s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequence output (BERT): -12.351841926574707, 13.68467903137207\n",
            "SINGLE LOSS: 2.5328455410564202e+20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Iteration:  30%|███       | 21/70 [01:10<02:41,  3.31s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequence output (BERT): -10.817895889282227, 13.075139999389648\n",
            "SINGLE LOSS: 2.532830763620143e+20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Iteration:  31%|███▏      | 22/70 [01:13<02:38,  3.29s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequence output (BERT): -11.410187721252441, 13.702019691467285\n",
            "SINGLE LOSS: 2.5328159861838656e+20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Iteration:  33%|███▎      | 23/70 [01:16<02:38,  3.38s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequence output (BERT): -10.212099075317383, 13.67501163482666\n",
            "SINGLE LOSS: 2.5328015605913092e+20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Iteration:  34%|███▍      | 24/70 [01:22<03:06,  4.06s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequence output (BERT): -11.759846687316895, 13.975378036499023\n",
            "SINGLE LOSS: 2.5327871349987528e+20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Iteration:  36%|███▌      | 25/70 [01:25<02:52,  3.84s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequence output (BERT): -11.249343872070312, 13.304853439331055\n",
            "SINGLE LOSS: 2.532772533484336e+20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Iteration:  37%|███▋      | 26/70 [01:31<03:07,  4.26s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequence output (BERT): -10.81851863861084, 13.409655570983887\n",
            "SINGLE LOSS: 2.532757931969919e+20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Iteration:  39%|███▊      | 27/70 [01:34<02:51,  3.99s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequence output (BERT): -12.343364715576172, 13.725702285766602\n",
            "SINGLE LOSS: 2.5327435063773626e+20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Iteration:  40%|████      | 28/70 [01:38<02:44,  3.91s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequence output (BERT): -13.245479583740234, 13.066840171813965\n",
            "SINGLE LOSS: 2.5327290807848062e+20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Iteration:  41%|████▏     | 29/70 [01:43<02:56,  4.29s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequence output (BERT): -10.5408296585083, 14.101422309875488\n",
            "SINGLE LOSS: 2.5327144792703894e+20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Iteration:  43%|████▎     | 30/70 [01:46<02:39,  3.98s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequence output (BERT): -8.50461196899414, 13.436603546142578\n",
            "SINGLE LOSS: 2.5326998777559725e+20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Iteration:  44%|████▍     | 31/70 [01:50<02:33,  3.94s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequence output (BERT): -12.431669235229492, 12.71997356414795\n",
            "SINGLE LOSS: 2.532685452163416e+20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Iteration:  46%|████▌     | 32/70 [01:54<02:28,  3.92s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequence output (BERT): -11.698928833007812, 12.521946907043457\n",
            "SINGLE LOSS: 2.5326710265708596e+20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Iteration:  47%|████▋     | 33/70 [01:57<02:17,  3.73s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequence output (BERT): -11.477239608764648, 14.139301300048828\n",
            "SINGLE LOSS: 2.5326564250564428e+20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Iteration:  49%|████▊     | 34/70 [02:01<02:11,  3.66s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequence output (BERT): -11.5579833984375, 12.801414489746094\n",
            "SINGLE LOSS: 2.532641823542026e+20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Iteration:  50%|█████     | 35/70 [02:04<02:02,  3.50s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequence output (BERT): -11.6135835647583, 14.076586723327637\n",
            "SINGLE LOSS: 2.5326273979494695e+20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Iteration:  51%|█████▏    | 36/70 [02:07<01:53,  3.33s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequence output (BERT): -10.761434555053711, 14.052170753479004\n",
            "SINGLE LOSS: 2.532612972356913e+20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Iteration:  51%|█████▏    | 36/70 [02:10<02:02,  3.62s/it]\n",
            "Epoch:   3%|▎         | 1/30 [07:25<3:35:33, 445.97s/it]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-aa3b951961ef>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m                 \u001b[0mtorchmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMeanAbsoluteError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m                 torchmetrics.MeanSquaredError().to(device)]\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclassify_and_count_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerate_preds_labels_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_quantization_preds_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdist_metrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-37-503eba1c1333>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(device, train_dataset, model, eval_dataset, generate_preds_labels_fn, metrics, num_train_epochs, train_batch_size, eval_batch_size, weight_decay, learning_rate, adam_epsilon, warmup_steps, max_grad_norm)\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"SINGLE LOSS: {loss}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_grad_norm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m             \u001b[0mepoch_loss_sum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmetric\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch_xla/_patched_functions.py\u001b[0m in \u001b[0;36mclip_grad_norm_\u001b[0;34m(parameters, max_norm, norm_type, error_if_nonfinite, foreach)\u001b[0m\n\u001b[1;32m     41\u001b[0m     total_norm = torch.norm(\n\u001b[1;32m     42\u001b[0m         torch.stack(\n\u001b[0;32m---> 43\u001b[0;31m             [torch.norm(p.grad.detach(), norm_type) for p in parameters]),\n\u001b[0m\u001b[1;32m     44\u001b[0m         norm_type)\n\u001b[1;32m     45\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0merror_if_nonfinite\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtotal_norm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtotal_norm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misinf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch_xla/_patched_functions.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     41\u001b[0m     total_norm = torch.norm(\n\u001b[1;32m     42\u001b[0m         torch.stack(\n\u001b[0;32m---> 43\u001b[0;31m             [torch.norm(p.grad.detach(), norm_type) for p in parameters]),\n\u001b[0m\u001b[1;32m     44\u001b[0m         norm_type)\n\u001b[1;32m     45\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0merror_if_nonfinite\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtotal_norm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtotal_norm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misinf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/functional.py\u001b[0m in \u001b[0;36mnorm\u001b[0;34m(input, p, dim, keepdim, out, dtype)\u001b[0m\n\u001b[1;32m   1630\u001b[0m             \u001b[0m_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2.0\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1631\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1632\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvector_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1633\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1634\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvector_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# metrics = [torchmetrics.Accuracy(num_classes=3, average='macro', task = \"multiclass\"),\n",
        "#      torchmetrics.F1Score(num_classes=3, average='macro', task = \"multiclass\")]\n",
        "# history = train(device=device, train_dataset=train_ds, model=wrapper, eval_dataset=val_ds,\n",
        "#                generate_preds_labels_fn=classification_preds_labels_fn, metrics=metrics)\n",
        "# _ , test_metric = evaluate(device = device, model = model,eval_dataset= test_ds, eval_batch_size= 32,\n",
        "# gen_preds_labels_fn=classification_preds_labels_fn)\n",
        "losses = {\"kldiv\": kldiv_loss,\n",
        "          \"mae\" : mae_loss}\n",
        "quantify_models = []\n",
        "for loss_name, loss in losses.items():\n",
        "    num_quantify_classes = 3\n",
        "    quantify_config = BertConfig.from_pretrained(\n",
        "        model_name_or_path,\n",
        "        num_labels=num_quantify_classes)\n",
        "    quantify_model = BertForLabelDistribution.from_pretrained(model_name_or_path, config=quantify_config, loss_fn=loss, loss_fn_name =loss_name )\n",
        "    quantify_model.to(device)\n",
        "    quantify_models.append(quantify_model)\n",
        "\n",
        "histories = {}\n",
        "test_metrics = {}\n",
        "for quantify_model in quantify_models:\n",
        "    qua_metrics = [torchmetrics.KLDivergence(log_prob=False, reduction=\"mean\").to(device),\n",
        "                torchmetrics.MeanAbsoluteError().to(device),\n",
        "                torchmetrics.MeanSquaredError().to(device)]\n",
        "    history = train(device=device, train_dataset=train_ds, model=quantify_model, eval_dataset=val_ds,\n",
        "                    generate_preds_labels_fn=get_quantization_preds_labels, metrics=qua_metrics)\n",
        "    histories[quantify_model.name] = history\n",
        "    test_metric = evaluate(device, quantify_model, test_ds, eval_batch_size=32, metrics=qua_metrics,\n",
        "                            gen_preds_labels_fn=get_quantization_preds_labels)\n",
        "    test_metrics[quantify_model.name] = test_metric"
      ],
      "metadata": {
        "id": "fEpnFYHHpZSp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}